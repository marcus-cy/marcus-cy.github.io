<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="Marcus Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Marcus Blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Marcus Blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Marcus'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>Marcus Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Marcus Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com2019/02/05/dataanalysis/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Marcus Anthony">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/mar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Marcus Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2019/02/05/dataanalysis/" itemprop="url">数据挖掘基础</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-02-05T19:30:33+08:00">
                2019-02-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="缺失处理"><a href="#缺失处理" class="headerlink" title="缺失处理"></a>缺失处理</h2><h3 id="数据缺失及其类别"><a href="#数据缺失及其类别" class="headerlink" title="数据缺失及其类别"></a>数据缺失及其类别</h3><ul>
<li>完全随机缺失：<br>当某变量缺失值发生的可能性与其他变量无关也与该变量自身无关时称作MCAR。换句话说，某变量缺失值的出现完全是个随机事件。可以将存在MCAR变量的数据看做是假定完整数据的一个随机样本<br>例如，由于测量设备出故障导致某些值缺失。</li>
<li>随机缺失：<br>当某变量出现缺失值的可能性与模型中某些观测变量有关而与该变量自身无关时称作MAR<br>例如，在一次测试中，如果IQ达不到最低要求的100分，那么将不能参加随后的人格测验。在人格测验上因为IQ低于100分而产生的缺失值为MAR</li>
<li>非随机缺失：<br>当某变量出现缺失值的可能性只与自身相关时称作MANR<br>例如，公司新录用了20名员工，由于6名员工表现较差在试用期内辞退，试用期结束后的表现评定中，辞退的6名员工的表现分即为非随机缺失。</li>
</ul>
<h3 id="数据缺失的处理方法"><a href="#数据缺失的处理方法" class="headerlink" title="数据缺失的处理方法"></a>数据缺失的处理方法</h3><ul>
<li>删除法：当一条记录中存在许多缺失时，将记录删去；当一个列存在许多缺失时，将列删去。或将特征映射到高维，对特征的每个属性做是否判断。或添加缺失项这一特征</li>
<li>均值替代法：使用每个变量的均值去填补该变量的缺失值。这种方法容易产生估计偏差。</li>
<li>回归法 ：根据变量间的相关，利用其他变量的信息通过建立回归方式去推算缺失值。该法同样会产生估计偏差。</li>
<li>期望值最大化（Expectation Maximization, EM）法：随机缺失的条件下，假设模型对于完整的样本是正确的，那么通过观测数据的边际分布可以对未知参数进行极大似然估计。这种方法也被称为忽略缺失值的极大似然估计，对于极大似然的参数估计实际中常采用的计算方法是期望值最大化。</li>
<li>K最近距离邻（K-means clustering）法：先确定距离具有缺失数据样本最近的K个样本，将这K个值加权平均来估计该样本的缺失数据。</li>
<li>不处理：把缺失当成一种特殊值。</li>
<li>拉格朗日插值法</li>
</ul>
<p>拉格朗日插值法，即对于点 $(x_1,y_1),(x_2,y_2),(x_3,y_3)​$<br>存在曲线经过这些点，运用线代中的坐标基向量的找法，该多项式曲线为 </p>
<script type="math/tex; mode=display">
f(x)=y_1f_1(x)+y_2f_2(x)+y_3f_3(x)+...</script><p>其中$f_i(x_j)= \begin{cases} 1, &amp; \text {i=j}   \\ 0 , &amp; \text{i!=j}    \end{cases} ​$</p>
<p>推导有 $ f_i(x)=\prod \cfrac{x-x_i}{x_i-x_j}  ​$</p>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><h3 id="特征选择的方法"><a href="#特征选择的方法" class="headerlink" title="特征选择的方法"></a>特征选择的方法</h3><p>前向选择法：在前一轮产生的最优特征集合上，添加这一轮的最优特征，且产生的特征集合优于上一轮。若在K+1轮时，最优的{K+1}特征集合表现不如上一轮的K个特征集，则停止搜索。<br>后向选择法：类似前向，前向选择和后向选择都属于贪心策略，仅考虑本轮的最优。对于特征选择方法，可以基于决策树算法，选取信息增益最大的特征。<br>过滤式选择：先进行特征选取，特征的选取依据一种相关统计量来度量，类似于距离度量。然后再训练学习器。<br>包裹式选择：将最终确定的学习器性能作为特征选择的评价准则。<br>嵌入式选择：将特征选择和学习器训练过程融为一体，即在损失函数中，为避免样本特征多，样本量少，产生的过拟合，引入正则化项，其中L1范数更容易获得稀疏解</p>
<h3 id="多重共线性检测"><a href="#多重共线性检测" class="headerlink" title="多重共线性检测"></a>多重共线性检测</h3><p>使用方差膨胀因子(VIF)来鉴定，如果VIF大于5，则说明变量存在多重共线性。一旦发现变量之间存在多重共线性的话，可以考虑删除变量和重新选择模型（岭回归法）</p>
<p>$VIF =\cfrac{1}{1-R^2}$<br>$R^2是以x_j$为因变量时对其它自变量回归的复测定系数</p>
<p><code>from statsmodels.stats.outliers_influence import variance_inflation_factor</code><br><code>variance_inflation_factor(data.values, index of the variable in the columns)</code></p>
<h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><p><code>fancyimpute 支持填充方法删除法、均值法、回归法、KNN、MICE、EM等</code></p>
<h2 id="统计知识"><a href="#统计知识" class="headerlink" title="统计知识"></a>统计知识</h2><h3 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h3><p>对总体参数做一个尝试性的假设，来确定是否应拒绝总体参数值</p>
<p>1.假设检验中的两类错误</p>
<blockquote>
<p>第一类错误：原假设为真，拒绝原假设<br>第二类错误：备择假设为真，不拒绝原假设</p>
</blockquote>
<p>2.显著性水平：犯第一类错误的概率</p>
<p>3.下侧检验：拒绝域出现在下侧</p>
<script type="math/tex; mode=display">
原假设：H_0:u≥u_0  

备择假设：H_a:u＜u_0</script><p>4.上侧检验：拒绝域出现在上侧</p>
<script type="math/tex; mode=display">
原假设：H_0:u≤u_0  

备择假设：H_a:u＞u_0</script><p>总体方差$\sigma^2$ 已知的情况下：Z检验<br>总体方差$\sigma^2$ 未知的情况下：t检验</p>
<p>5.p值：概率值，是样本所提供的证据对原假设支持程度的度量，p越小，表明反对原假设的证据越多,p大于显著性水平时，不拒绝原假设</p>
<h3 id="自由度"><a href="#自由度" class="headerlink" title="自由度"></a>自由度</h3><ul>
<li>wiki解释</li>
</ul>
<p>　　<strong>在统计学中，自由度（英语：degree of freedom, df）是指当以样本的统计量来估计总体的参数时，样本中独立或能自由变化的数据的个数，称为该统计量的自由度[1]。一般来说，自由度等于独立变量减掉其衍生量数[2]；举例来说，方差的定义是样本减平均值的平方之和（一个由样本决定的衍生量），因此对N个随机样本而言，其自由度为N-1。</strong>  </p>
<p>　　<strong>估计总体的方差时所使用的统计量是样本的方差 s，而 s必须用到样本平均数x来计算。 x在抽样完成后已确定，所以大小为 n的样本中只要  n-1个数确定了，第 n个数就只有一个能使样本符合x的数值。也就是说，样本中只有  n-1个数可以自由变化，只要确定了这  n-1个数，方差也就确定了。这里，平均数 x就相当于一个限制条件，由于加了这个限制条件，样本方差 s的自由度为 n-1。</strong></p>
<h3 id="中心极限定理"><a href="#中心极限定理" class="headerlink" title="中心极限定理"></a>中心极限定理</h3><p>若给定样本量的所有样本来自任意总体，则<strong>样本均值的抽样分布近似服从正态分布</strong>，样本量越大，近似性越强</p>
<h3 id="方差和标准差"><a href="#方差和标准差" class="headerlink" title="方差和标准差"></a>方差和标准差</h3><p>样本方差<script type="math/tex">s^2</script>是对总体方差<script type="math/tex">\sigma ^2</script>的无偏估计</p>
<h3 id="Pearson相关系数"><a href="#Pearson相关系数" class="headerlink" title="Pearson相关系数"></a>Pearson相关系数</h3><p>两个变量之间是线性关系，总体是正态分布<br>$  r=\cfrac{\sum (X-\overline{X})(Y-\overline{Y})}{\sqrt{\sum (X-\overline X})^2 \sqrt{\sum (Y-\overline Y})^2} $</p>
<h3 id="超几何分布"><a href="#超几何分布" class="headerlink" title="超几何分布"></a>超几何分布</h3><p>N中有M个特定种类，抽取n个时，会有k个特定种类的概率</p>
<h3 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h3><p>导数，指的是一元函数中，函数y=f(x)在某一点处沿x轴正方向的变化率； </p>
<p><img src="/2019/02/05/dataanalysis/20160325131644664.jpg" alt=""></p>
<h3 id="方向导数"><a href="#方向导数" class="headerlink" title="方向导数"></a>方向导数</h3><p>方向导数的定义，即：某一点在某一趋近方向上的变化率，就是函数在其他特定方向上的变化率</p>
<p><img src="/2019/02/05/dataanalysis/20160325132224619.jpg" alt=""></p>
<h3 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h3><p>偏导数，指的是多元函数中，函数y=f(x1,x2,…,xn)在某一点处沿某一坐标轴（x1,x2,…,xn）正方向的变化率。 </p>
<p><img src="/2019/02/05/dataanalysis/20160325132018016.jpg" alt="">
　</p>
<h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>梯度为解决空间中的某点，沿着哪个方向有最大的变化率</p>
<script type="math/tex; mode=display">gradf(x_0,x_1,...,x_n)=(\cfrac{\delta{f}}{\delta{x_0}},...,\cfrac{\delta{f}}{\delta{x_n}})</script><p>既有<br>1.梯度是一个向量，即有方向有大小；<br>2.梯度的方向是最大方向导数的方向；<br>3.梯度的值是最大方向导数的值。 </p>
<h3 id="泰勒公式"><a href="#泰勒公式" class="headerlink" title="泰勒公式"></a>泰勒公式</h3><script type="math/tex; mode=display">若f(x)在a点处n+1可导，那么函数在a处的泰勒展开式</script><script type="math/tex; mode=display">f(x)=f(a)+\cfrac{f'(a)}{1!}(x-a)+\cfrac{f^{(2)}(a)}{2!}(x-a)^2+..+\cfrac{f^{(n)}(a)}{n!}(x-a)^n+R_n</script><p>其中<script type="math/tex">R_n</script>是泰勒公式的余项，是<script type="math/tex">(x-a)^n</script>的高阶无穷小</p>
<h3 id="仿射"><a href="#仿射" class="headerlink" title="仿射"></a>仿射</h3><p>任意两点<script type="math/tex">X_1,X_2 \in K 以及 \alpha \in R</script>，这两点连线上的所有点<script type="math/tex">\alpha X_1+(1-\alpha)X_2 \in K</script>,则称这些点为仿射组合</p>
<h3 id="仿射函数"><a href="#仿射函数" class="headerlink" title="仿射函数"></a>仿射函数</h3><p>仿射函数即由 1 阶多项式构成的函数，一般形式为 f (x) = A x + b，这里，A 是一个 m×k 矩阵，x 是一个 k 向量，b 是一个 m 向量，实际上反映了一种从 k 维到 m 维的空间映射关系</p>
<h3 id="超平面"><a href="#超平面" class="headerlink" title="超平面"></a>超平面</h3><p>超平面H是从n维空间到n-1维空间的一个映射子空间，它有一个n维向量和一个实数定义。因为是子空间，所以超平面一定过原点，二维空间里的超平面为一条直线. 一维空间里超平面为数轴上的一个点。</p>
<h3 id="凸集"><a href="#凸集" class="headerlink" title="凸集"></a>凸集</h3><p>K是n维欧式空间一点集合，若任意两点<script type="math/tex">X_1,X_2 \in K</script>，这两点连线上的所有点<script type="math/tex">\alpha X_1+(1-\alpha)X_2 \in K ,(0 \leq\alpha\leq1)</script>,则称<script type="math/tex">K</script>为凸集</p>
<h3 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h3><p>定义在某个向量空间的凸子集区间上的实值函数，对于任意两点<script type="math/tex">x,y,以及t \in [0,1]</script>，有<br>$ f(tx+(1-t)y) \leq tf(x)+(1-t)f(y) $，也就是凸函数上任意两点的连线都在两点区间的函数之上   </p>
<p>凸函数的性质：</p>
<blockquote>
<p>1.凸函数的epigraph（函数图上或上方的点集合）为凸集<br>2.对于<script type="math/tex">∀x,y，f(y)≥f(x)+∇f(x)^T(y−x)</script>,即在函数某点切线上的点的值小于该点在函数上的值<br>3.对于<script type="math/tex">∀x∈dom(f)， ∇^2f(x)≥0</script>, 函数图像在定义域中每点处均有向上的正弯曲<br>4.凸函数的任何 极小值都是最小值，严格凸函数最多有一个最小值<br>5.凸函数具有仿射不变性，即f(x)是凸函数，则g(y)=f(Ax+b)也是凸函数</p>
</blockquote>
<h3 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h3><blockquote>
<p>1.目标函数必须是凸的<br>2.不等式约束函数必须是凸的<br>3.等式约束函数必须是仿射的</p>
</blockquote>
<h3 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h3><p>概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。</p>
<p>对于P(x|θ)，x表示某一个具体的数据；θ表示模型的参数。</p>
<p>如果θ是已知确定的，x是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点x，其出现概率是多少。</p>
<p>如果x是已知确定的，θ是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现x这个样本点的概率是多少。</p>
<h3 id="对数几率"><a href="#对数几率" class="headerlink" title="对数几率"></a>对数几率</h3><p>一个事件发生的概率与不发生的概率的比值称为几率<br>对数几率函数为 <script type="math/tex">logit(P)=log(\cfrac{P}{1-P})=\beta_0+\beta_1x_1+...+\beta_nx_n</script>,与逻辑回归函数转化</p>
<h2 id="线性代数1"><a href="#线性代数1" class="headerlink" title="线性代数1"></a>线性代数1</h2><h3 id="二维空间的向量"><a href="#二维空间的向量" class="headerlink" title="二维空间的向量"></a>二维空间的向量</h3><p>可以由两个基向量$ \vec{i}=\left[ \begin{array}{ccc}1\\0<br>\end{array}\right]<br>,\vec{j}=\left[ \begin{array}{ccc}0\\1<br>\end{array}\right]<br> $，经过向量数乘和向量加法运算后表示<br>$\left[ \begin{array}{ccc}<br>-3\\<br>2<br>\end{array}\right]<br>=-3\vec{i}+2\vec{j}$<br>向量空间的一组基是张成该空间的一个线性无关向量集，当选择不同的基向量时，获得一个新的坐标系</p>
<h3 id="线性组合"><a href="#线性组合" class="headerlink" title="线性组合"></a>线性组合</h3><p>两个数乘向量的和被称为这两个向量的线性组合<br>$a\vec{v}+b\vec{w},即\vec{v}和\vec{w}的线性组合，构成的向量集合称为张成的空间$<br>当固定其中一个标量，让另一个标量自由变化，所产生的向量的终点会描出一条直线；<br>当两个标量同时自由变化（非零向量；不共线的情况下），可以获得所有的二维向量</p>
<h3 id="线性变换"><a href="#线性变换" class="headerlink" title="线性变换"></a>线性变换</h3><p>接受一个向量并且输出一个向量的变换</p>
<p>线性变换的特性：<br>1.直线依旧是直线；<br>2.原点保持固定</p>
<p>考虑基向量线性变换后的位置，其他向量通过变换后基向量表示变换值<br>变换后的基向量</p>
<script type="math/tex; mode=display">
\vec{i}=\left[ \begin{array}{ccc}1\\-2
\end{array}\right]
,\vec{j}=\left[ \begin{array}{ccc}3\\0
\end{array}\right]
，则变换后的
\vec{v}=-3\vec{i}+2\vec{j}=\left[ \begin{array}{ccc}
-3*1+2*3\\
-3*-2+2*0
\end{array}\right]</script><h3 id="矩阵乘法和线性变换"><a href="#矩阵乘法和线性变换" class="headerlink" title="矩阵乘法和线性变换"></a>矩阵乘法和线性变换</h3><script type="math/tex; mode=display">
A=\left[ \begin{array}{ccc}i,j
\end{array}\right]=\left[ \begin{array}{ccc}1 & 3\\-2 & 0
\end{array}\right]是变换后的基向量，\left[ \begin{array}{ccc}x \\y
\end{array}\right] 是任意初始向量，则A\left[ \begin{array}{ccc}x \\y
\end{array}\right]为线性变换后的向量</script><h2 id="线性代数2"><a href="#线性代数2" class="headerlink" title="线性代数2"></a>线性代数2</h2><h3 id="矩阵乘法和线性变换-续"><a href="#矩阵乘法和线性变换-续" class="headerlink" title="矩阵乘法和线性变换(续)"></a>矩阵乘法和线性变换(续)</h3><p>两个矩阵相乘的几何意义，就是两次线性变换的相继作用<br>$AB \begin{bmatrix} x \\ y \end{bmatrix}=C \begin{bmatrix} x \\y \end{bmatrix}$</p>
<h3 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h3><p>矩阵A的行列式$det(A)=M$,表示将一个区域的面积变换为原来的M倍，若M为0，则表示线性变换将平面压缩成一条线或者是一个点。即通过判断行列式是否为0，确定矩阵所代表的线性变换是否将空间压缩，降维 </p>
<h3 id="线性方程组"><a href="#线性方程组" class="headerlink" title="线性方程组"></a>线性方程组</h3><p>对于求解线性方程组$ A\vec{x}=\vec{v},即是找到一个向量\vec{x}，在经过矩阵A线性变换后，与\vec{v}重合$</p>
<h3 id="逆矩阵"><a href="#逆矩阵" class="headerlink" title="逆矩阵"></a>逆矩阵</h3><p>对于$A^{-1}A=E$的恒等变换 ，只存在于det(A)≠0，即若矩阵A代表的线性变换将空间压缩，则无法通过逆变换还原</p>
<h3 id="秩"><a href="#秩" class="headerlink" title="秩"></a>秩</h3><p>代表着线性变换后空间的维数，当列数与秩相等时，称为满秩</p>
<h3 id="向量的点积"><a href="#向量的点积" class="headerlink" title="向量的点积"></a>向量的点积</h3><p>向量的点积<script type="math/tex">\vec{v}*\vec{w}=\vec{v}的投影长度*\vec{w}的长度=\vec{v}*\vec{w}的投影长度</script></p>
<p>对偶性:两种数学事物之间自然而又出乎意料的对应关系</p>
<h3 id="向量的叉积"><a href="#向量的叉积" class="headerlink" title="向量的叉积"></a>向量的叉积</h3><p>$ \vec{v}\times\vec{w}=\vec{p},其中\vec{p}的长度等于平行四边形的面积，方向垂直于\vec{v}和\vec{w} ​$</p>
<h2 id="线性代数3"><a href="#线性代数3" class="headerlink" title="线性代数3"></a>线性代数3</h2><h3 id="不同坐标系下的基向量"><a href="#不同坐标系下的基向量" class="headerlink" title="不同坐标系下的基向量"></a>不同坐标系下的基向量</h3><p>A坐标系下基向量<script type="math/tex">\vec{i},\vec{j}</script>，B坐标系下基向量<script type="math/tex">\vec{b_1},\vec{b_2} 为\left[ \begin{array}{ccc}0 \\1
\end{array}\right] 和\left[ \begin{array}{ccc}1 \\0
\end{array}\right]</script>，其中从A的视角中，<script type="math/tex">\vec{b_1}=\left[ \begin{array}{ccc}2 \\1
\end{array}\right],\vec{b_2}=\left[ \begin{array}{ccc}-1 \\1
\end{array}\right]</script>，这两个坐标系都共用一个原点，但是坐标轴的方向和网格间距会不同，这依赖与对基的选择</p>
<p>令<script type="math/tex">M=\left[ \begin{array}{ccc}2 & -1 \\1 &1
\end{array}\right]​</script>,对于两种坐标系下的向量转换，即存在<script type="math/tex">M\alpha=\beta ​</script>,其中 <script type="math/tex">\alpha=\left[ \begin{array}{ccc}1 & 0 \\0 &1
\end{array}\right] ​</script> 为B坐标系视角下的向量，<script type="math/tex">\beta=\left[ \begin{array}{ccc} \vec{b_1} & \vec{b_2}
\end{array}\right]  ​</script>为A坐标系视角下的向量。</p>
<p>推广下，则有<br>$  M^{-1}PM\alpha 代表在两种坐标系视角下，经过矩阵P(A视角下)的线性变换后，获得的B视角下的向量 ​$</p>
<h3 id="特征向量"><a href="#特征向量" class="headerlink" title="特征向量"></a>特征向量</h3><p>对于线性变化A，若存在原向量$ \alpha $,经过A变化后，仍在其张成的空间中，只做拉伸变化，即存在<br>$ A\alpha=\lambda\alpha $，其中$ \lambda $为拉伸或压缩比例因子，称向量$ \alpha为特征向量，\lambda为特征值 $</p>
<p>特征向量的用途：在一个三维空间旋转中，对于旋转的特征向量，可以将此特征向量视为旋转轴，此时特征值为1</p>
<h3 id="特征基"><a href="#特征基" class="headerlink" title="特征基"></a>特征基</h3><p>当特征向量可作为基向量时，即<script type="math/tex">\vec{b_1},\vec{b_2}</script> 为P变换中的特征向量，则有</p>
<script type="math/tex; mode=display">M^{-1}PM 为新基的视角下P变换，所得到的矩阵，这个矩阵为对角矩阵，对角元为特征值，其中M为P变化的特征向量，这是因为在所处的坐标系的基向量在变换中只进行了伸缩</script><h3 id="正交矩阵"><a href="#正交矩阵" class="headerlink" title="正交矩阵"></a>正交矩阵</h3><p>矩阵的逆和矩阵的转置相等</p>
<h3 id="正定矩阵"><a href="#正定矩阵" class="headerlink" title="正定矩阵"></a>正定矩阵</h3><p>对任意非零向量<script type="math/tex">\vec{x},有x^TAx>0</script>,即称A为正定矩阵，且A的特征值均为正；<br>正定、半正定矩阵的直觉代表一个向量经过它的变化后的向量与其本身的夹角小于等于90度。</p>
<h3 id="二次型矩阵"><a href="#二次型矩阵" class="headerlink" title="二次型矩阵"></a>二次型矩阵</h3><p>二次型矩阵具有对称性，所以二次型一定可以化成对角矩阵</p>
<h3 id="相似对角化"><a href="#相似对角化" class="headerlink" title="相似对角化"></a>相似对角化</h3><p>如果矩阵与一个对角阵相似，那么他们具有相同的特征多项值、特征值、秩<br>一个矩阵 <script type="math/tex">A=\lambda_1P_1+\lambda_2P_2+...</script><br>其中，<script type="math/tex">P称为A的谱族，\lambda为特征值,特征值
和谱族的乘积就代表了它对矩阵A 的贡献率,即能量（power）或者权重
(weight)。这样，能量多的、权重大的部分当然重要，问题的主要矛盾就凸显出来了</script><br>对于矩阵<script type="math/tex">A=P\left[ \begin{array}{ccc}\lambda_1 &  \\ & \lambda_2
\end{array}\right]P^{-1}，其中P=[p_1,p_2,..],P^{-1}=[q_1,q_2,..]^T,则有P_1=p_1q_1,...</script>,</p>
<h2 id="线性回归方程"><a href="#线性回归方程" class="headerlink" title="线性回归方程"></a>线性回归方程</h2><blockquote>
<p>简单线性回归模型</p>
</blockquote>
<script type="math/tex; mode=display">
y=β_0 + β_1 X + ε</script><p>ε 是一个随机变量，称为模型的误差项，即不能被x，y之间的线性关系解释的变异性</p>
<blockquote>
<p>简单线性回归方程</p>
</blockquote>
<script type="math/tex; mode=display">
E(y)=β_0 + β_1 X</script><blockquote>
<p>估计的简单线性回归方程</p>
</blockquote>
<script type="math/tex; mode=display">
\hat{y}=b_0 + b_1 X</script><p>用样本统计量 <script type="math/tex">b_0 + b_1 X</script> 替代回归方程中的未知参数</p>
<blockquote>
<p>最小二乘法准则   </p>
</blockquote>
<p>最小二乘法是利用样本数据，通过使应变量的观测值<script type="math/tex">y_i</script>与应变量的预测值 <script type="math/tex">\hat{y}</script> 之间的离差平方和达到最小的方法求得 <script type="math/tex">b_0 和 b_1</script> </p>
<script type="math/tex; mode=display">
min\Sigma(y_i-\hat{y})^2</script><blockquote>
<p>判定系数<br>为估计回归方程提供一个拟合优度的度量  </p>
</blockquote>
<p>1.离差$ y_i -\hat{y} <script type="math/tex">表示用方程的估计值预测所产生的误差，称为误差平方和</script>  SSE=\Sigma(y_i -\hat{y})^2 $</p>
<p>2.离差$  y_i -\bar{y} <script type="math/tex">表示用样本平均值来去预测所产生的误差，称为总平方和</script> SST=\Sigma(y_i -\bar{y})^2 $</p>
<p>3.离差$  \hat{y} -\bar{y} $ ，称为回归平方和 $ SSR=\Sigma(\hat{y} -\bar{y})^2 $</p>
<p>4.$SST=SSR+SSE​$</p>
<p>5.判定系数：$ r^2= \frac {SSR} {SST} $</p>
<p>6.仅仅依靠判定系数，无法判断x和y之间的显著性，只能判断最小二乘回归直线是否很好的拟合了样本数据</p>
<blockquote>
<p>相关系数<br>$  r_{xy}=（b_1的符号)\sqrt{判定系数} $<br>样本相关系数被限制在两变量之间存在线性关系的情况</p>
</blockquote>
<h3 id="显著性检验"><a href="#显著性检验" class="headerlink" title="显著性检验"></a>显著性检验</h3><p>关于简单线性回归模型中误差项ε的假定</p>
<script type="math/tex; mode=display">
y=β_0 + β_1 X + ε</script><p>1.误差项ε是一个期望值为零的随机变量，即$E(y)=0$,<br>即有 $E(y)=β_0 + β_1 X$</p>
<p>2.对所有的x值，ε的方差$\sigma^2 $相同</p>
<p>3.ε值是独立的，且符合正态分布</p>
<p>4.假设检验，判定$β_1$ 是否为0</p>
<p>$\sigma^2 $的估计：$ s^2=MSE=\frac{SSE} {n-2} $</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>最小二乘法的拟合误差函数：<br>$ loss=\Sigma(y_i-\hat{y})^2=\Sigma(y_i-β_0 - β_1 X)^2 $<br>损失函数：<br>$ Loss function= Fit Error + Model Complexity $<br>其中复杂度代价函数就是约束我们的模型尽量的简单，模型越复杂，规则化值越大</p>
<blockquote>
<p>范数</p>
</blockquote>
<p>范数，是具有“长度”概念的函数。<br>L1范数对应曼哈顿距离<br>L2范数对应欧式距离</p>
<blockquote>
<p>L0范数</p>
</blockquote>
<p>向量中非零元素的个数</p>
<blockquote>
<p>L1范数 (Lasso regularization) </p>
</blockquote>
<p>$ ||x||_1 = \sum_{i=1}^N|x_i| $，即向量元素绝对值之和<br>L1正则化有助于生成一个稀疏的权值矩阵，即使线性回归模型中大多数系数为0<br>损失函数：<br>$ J=J_0+\alpha\sum|w| $<br>其中$J_0为原始的损失函数，\alpha为正则化系数 $</p>
<blockquote>
<p>L2范数 (Ridge Regression)</p>
</blockquote>
<p>$ ||x||_2 = \sqrt[2]{\sum_{i=1}^N|x_i^2|} $，即向量元素绝对值平方和的开平方</p>
<p><strong>L1范数和L2范数正则化都有助于降低过拟合风险，但前者更易于获得更少的非零向量。即L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0</strong></p>
<p>如图，假定x仅有两个属性，此时L1范数等值线和平方误差等值线交点更常出现在坐标轴上，即参数为0的情况<br><img src="/2019/02/05/dataanalysis/1527426007.png" alt="1527426007"></p>
<h3 id="参数优化"><a href="#参数优化" class="headerlink" title="参数优化"></a>参数优化</h3><p>机器学习的参数估计，是寻找最优参数使得损失函数最小化。在理想的情况下，损失函数是凸的、连续的、可导的。因此最基本的优化算法有：</p>
<blockquote>
<p>梯度下降法(Gradient Descend Method)：     </p>
</blockquote>
<p>梯度下降法，用当前负梯度的方向作为搜索方向，梯度下降法算法用梯度乘以一个称为学习速率（有时也称为步长）的标量，以确定下一个点的位置<br>对于损失函数<script type="math/tex">J(\theta)</script>,其中m为样本个数,对<script type="math/tex">\theta</script>求导，<br><img src="/2019/02/05/dataanalysis/p1.png" alt=""><br><img src="/2019/02/05/dataanalysis/p2.jpg" alt=""><br><img src="/2019/02/05/dataanalysis/640.webp" alt=""></p>
<blockquote>
<p>批量梯度下降法(BGD)</p>
</blockquote>
<p>对于BGD法，每次迭代需要把m个样本全部带入计算，可以得到全局最优解，但会导致计算量庞大</p>
<blockquote>
<p>随机梯度下降法(SGD)</p>
</blockquote>
<p>在更新每一参数时都使用一个样本来进行更新，也就是m等于1。每一次跟新参数都用一个样本。最终的结果往往是在全局最优解附近，适用于大规模训练样本</p>
<blockquote>
<p>小批量梯度下降</p>
</blockquote>
<p>是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效</p>
<blockquote>
<p>牛顿法和拟牛顿法： </p>
</blockquote>
<p>由<script type="math/tex">f(x)在x_k点处的</script>泰勒公式推展，对于极小值点有<script type="math/tex">f'(x)=0</script>,既有<script type="math/tex">x=x_k-\cfrac{f'(x_k)}{f''(x_k)}</script>,即需要求值<script type="math/tex">d_k=-\cfrac{f'(x_k)}{f''(x_k)}</script>,令$ x_{k+1}=x_k+d_k $,重新迭代计算</p>
<p>梯度下降使用梯度(一阶导数)寻找下降最快的方向，而牛顿法通过曲率(二阶导数)寻找下降最快的方向，因此牛顿法的路径更直接。</p>
<blockquote>
<p>共轭梯度法：             </p>
</blockquote>
<p>介于最速下降法与牛顿法之间的一个方法，仅需一阶导数信息</p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><h3 id="logistic函数"><a href="#logistic函数" class="headerlink" title="logistic函数"></a>logistic函数</h3><p>$𝑓(𝑥)=\cfrac{𝑒𝑥𝑝(𝑥)}{1+exp⁡(𝑥)}$</p>
<p>函数特点:</p>
<blockquote>
<p>单调递增<br>0＜f(x)＜1<br>处处可导<br>$𝑓^′ (𝑥)=𝑓(𝑥)(1−𝑓(𝑥))$</p>
</blockquote>
<h3 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h3><p>(回归模型是对概率做回归，而非对结果做回归)：<br>$ p(Y=1|x)=\cfrac{𝑒𝑥𝑝(\beta𝑥)}{1+exp⁡(\beta𝑥)}$<br>$ p(Y=0|x)=\cfrac{1}{1+exp⁡(\beta𝑥)}$<br>$ P(Y_i=y_i)=p(Y=1|x)^{y_i}p(Y=0|x)^{1-y_i} $</p>
<h3 id="参数优化-1"><a href="#参数优化-1" class="headerlink" title="参数优化"></a>参数优化</h3><p>（极大似然法MLE）</p>
<p>似然函数为：</p>
<p>$ L(x,y|\beta)=\prod_{i=1}^N[p(Y=1|x)^{y_i}][p(Y=0|x)^{1-y_i}] $</p>
<p>对数似然函数为：<br>$ LL(x,y|\beta)=\sum_{i=1}^N[y_ilog(\cfrac{e^{\beta x}}{1+e^{\beta x}})+(1-y_i)log(\cfrac{1}{1+e^{\beta x}})]=\sum_{i=1}^N[y_i(\beta x_i)-log(1+exp(\beta x_i))] $</p>
<p>为使函数实现最大值，可采用梯度上升法，</p>
<p>$ \cfrac{\partial LL(x,y|\beta)}{\partial \beta_i}=\sum_{i=1}^N[y_ix_i-x_i\cfrac{exp(x_i\beta)}{1+exp(x_i\beta)}] $</p>
<p>$ \beta^{i+1}=\beta^{i}+\nabla*h $ (h为步长)</p>
<p>最优的参数值即为<br>$ \beta=\sum\nabla_i*h_i $</p>
<p>对于多分类情况，借用softmax函数，或者通过多个二分类实现</p>
<h3 id="模型特点"><a href="#模型特点" class="headerlink" title="模型特点"></a>模型特点</h3><p>模型的结果是估计出属于某一类的概率；<br>变量之间是线性可加关系；<br>可增量式读取数据；<br>需要对非数值变量进行编码；<br>对缺失值敏感，需要处理；<br>对异常值敏感，需要处理；<br>需要做变量归一化；<br>变量的线性相关性对模型有影响，需要变量挑选和正则化；</p>
<h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><h3 id="训练集与测试集的划分方法"><a href="#训练集与测试集的划分方法" class="headerlink" title="训练集与测试集的划分方法"></a>训练集与测试集的划分方法</h3><ul>
<li>留出法</li>
</ul>
<p>将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D=S∪T且S∩T=∅，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。</p>
<ul>
<li>交叉验证法</li>
</ul>
<p>交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10</p>
<ul>
<li>自助法(bootstrapping)</li>
</ul>
<p>对于含m个样本的数据集，开始有放回的抽取m次,作为训练集,然后把数据集和训练集的差集作为测试集，其中样本不被抽到的概率为<br><img src="/2019/02/05/dataanalysis/20170213172108998.jpg" alt=""></p>
<h3 id="度量标准"><a href="#度量标准" class="headerlink" title="度量标准"></a>度量标准</h3><p>衡量模型泛化能力的评价标准</p>
<ul>
<li>错误率和精度</li>
<li>查准率（precision）和查全率（recall）</li>
</ul>
<p><img src="/2019/02/05/dataanalysis/1528726169.jpg" alt=""></p>
<p>模型总体判断准确率</p>
<script type="math/tex; mode=display">Accuracy=\cfrac{TP+TN}{TP+TN+FP+FN}</script><p>查准率</p>
<script type="math/tex; mode=display">Precision=\cfrac{TP}{TP+FP}</script><p>查全率</p>
<script type="math/tex; mode=display">Recall=\cfrac{TP}{TP+FN}</script><p>查准和查全是一对矛盾的度量，查准率-查全率曲线，简称P-R曲线。<br>$ F_\beta=\cfrac{(1+\beta^2) \times P \times R}{(\beta^2 \times P)+R} $ 表达了查准率/查全率 的不同偏好，采用调和平均值，调和平均更重视较小值。其中 $\beta&gt;1$时查全率有更大的影响，小于1时查准率有更大影响</p>
<h3 id="ROC和AUC"><a href="#ROC和AUC" class="headerlink" title="ROC和AUC"></a>ROC和AUC</h3><p>ROC曲线是多个混淆矩阵的结果组合，如果在上述模型中我们没有定好阈值，而是将模型预测概率从高到低排序，将每个概率值依次作为阈值，那么就有多个混淆矩阵。<br>对于每个混淆矩阵，我们计算两个指标</p>
<script type="math/tex; mode=display">纵轴TPR（True positive rate）= TPR=\frac{TP}{(TP+FN)}=Recall</script><script type="math/tex; mode=display">横轴FPR（False positive rate）=\frac{FP}{(FP+TN)}</script><p>因此，ROC曲线越接近左上角，该分类器的性能越好。</p>
<p>AUC的值为ROC曲线下面的面积，一般AUC均在0.5到1之间，AUC越高，模型的区分能力越好，若AUC=0.5，表示模型的区分能力与随机猜测没有差别</p>
<h2 id="K近邻算法"><a href="#K近邻算法" class="headerlink" title="K近邻算法"></a>K近邻算法</h2><h3 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h3><p>存在一个训练样本集，并且样本集中每个数据都存在标签。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。</p>
<h3 id="kNN算法之KD树（K-dimension-tree）"><a href="#kNN算法之KD树（K-dimension-tree）" class="headerlink" title="kNN算法之KD树（K-dimension tree）"></a>kNN算法之KD树（K-dimension tree）</h3><p>KD树建树采用的是计算n个特征的取值的方差，用方差最大的特征值来作为根节点，该特征下的中位数作为划分点，对于所有第k维特征的取值小于划分点的样本，我们划入左子树，对于第k维特征的取值大于等于划分点的样本，我们划入右子树，对于左子树和右子树，我们采用和刚才同样的办法来找方差最大的特征来做更节点，递归的生成KD树。</p>
<h3 id="KNN的主要优点："><a href="#KNN的主要优点：" class="headerlink" title="KNN的主要优点："></a>KNN的主要优点：</h3><p>可用于分类和回归；<br>复杂度比SVM低；<br>适用样本量大的分类；</p>
<h3 id="KNN的主要缺点："><a href="#KNN的主要缺点：" class="headerlink" title="KNN的主要缺点："></a>KNN的主要缺点：</h3><p>特征数多时，计算量大；<br>可解释性不强</p>
<h2 id="K-means算法"><a href="#K-means算法" class="headerlink" title="K-means算法"></a>K-means算法</h2><h3 id="算法原理-1"><a href="#算法原理-1" class="headerlink" title="算法原理"></a>算法原理</h3><p>聚类是一种无监督的学习，它将相似的对象归到同一个簇中，簇内的对象越相似，聚类的效果越好。K-均值（K-means）聚类的算法是将k个不同的簇，且每个簇的中心采用簇中所含值的均值计算而成。<br>K-means算法：创建k个点作为起始质心（经常是随机选择），计算质心与每个数据点之间的距离，根据距离形成K个簇，重新计算K个簇的质心，形成新的簇，重复迭代计算，直至质心不变</p>
<h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><p>基本性质：<br>非负性 <script type="math/tex">dist(x_i,x_j)≥0</script><br>同一性 <script type="math/tex">dist(x_i,x_j)=0 当且仅当 x_i=x_j</script><br>对称性 <script type="math/tex">dist(x_i,x_j)=dist(x_j,x_i)</script><br>直递性 <script type="math/tex">dist(x_i,x_j)≤ dist(x_i,x_k)+dist(x_k,x_j)</script></p>
<ul>
<li>闵可夫斯基距离</li>
</ul>
<script type="math/tex; mode=display">dist(X_i,X_j)=(\Sigma |X_{ik}-X_{jk}|^P)^{\frac{1}{P}}</script><ul>
<li>欧几里得距离（p=2时）</li>
</ul>
<script type="math/tex; mode=display">dist(X_i,X_j)= \sqrt[2]{\Sigma|X_{ik}-X_{jk}|^2}</script><ul>
<li>曼哈顿距离（p=1时）</li>
</ul>
<script type="math/tex; mode=display">dist(X_i,X_j)= \Sigma|X_{ik}-X_{jk}|</script><ul>
<li>无序属性采用VDM距离</li>
</ul>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><p>1.K均值，基于原型的、划分的聚类技术，直至满足指定个数的簇<br>2.凝聚层次聚类，开始每个点作为簇，然后重复合并最近的簇，直至产生预设的簇个数<br>3.分裂层次聚类，从包含所有点的某个簇开始，每一步分裂一个簇，直至产生预设的簇个数<br>4.DBSCAN，基于密度的聚类算法，簇的个数由算法自动确定，低密度区域的点视为噪声而忽略，因此不会产生完全聚类</p>
<p>K-means算法对离群点敏感（需要进行离群点检测），不能处理非球形簇，不同尺寸，不同密度的簇。仅限于具有中心（质心）概念的数据。K-中心点可以弥补受离群点的影响</p>
<p>二分K均值<br>将所有集合分成两个簇，选取一个簇进行分裂，直到产生K个簇</p>
<h3 id="簇度量"><a href="#簇度量" class="headerlink" title="簇度量"></a>簇度量</h3><p>1.簇的非监督度量分成两类：簇的凝聚性来度量确定簇中对象如何密切相关，簇的分离性度量确定某个簇不同于其他簇的地方<br>2.簇的监督度量簇标号与标号的匹配程度</p>
<h3 id="轮廓系数（结合凝聚度和分离度两种因素）"><a href="#轮廓系数（结合凝聚度和分离度两种因素）" class="headerlink" title="轮廓系数（结合凝聚度和分离度两种因素）"></a>轮廓系数（结合凝聚度和分离度两种因素）</h3><p>1.某个点到簇中其他点的平均距离a<br>2.某个点到其他簇中点的距离b<br>3.某点的轮廓系数$ s=\cfrac{b-a}{max[a,b]} $</p>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>前馈神经网络是由一层一层构建的，涉及到输入层，隐藏层（中间层），输出层，每层的关系可以由公式<script type="math/tex">\vec{y}=f(w\vec{x}+b)</script> 表示，其中 f为激活函数，常用Sigmod函数表示。即可以理解为每层之间的信息的传递是通过线形变换和非线性变换。增加节点数：增加维度，即增加线性转换能力。增加层数：增加激活函数的次数，即增加非线性转换次数。</p>
<p>将网络的预测值和目标值之间的均方误差作为目标函数，采用梯度下降法，对权重参数调整</p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><h3 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h3><p>在划分数据集之前之后信息发生的变化称为信息增益，获得信息增益最高的特征就是最好的选择。信息熵与条件熵相减就是我们的信息增益。</p>
<p>信息量如何量化。信息量的大小跟事情不确定性的变化有关。而不确定性的变化，一，跟事情的可能结果的数量有关；二，跟概率有关，越小概率的事情发生了代表信息量越大，越大概率的事情发生了代表信息量越小。香农的信息量的公式为<script type="math/tex">log_2(X)</script>,例32的信息量为5bit，32支球队参加世界杯，通常只需要5五次，就可以得到答案。</p>
<p>信息熵就是平均而言发生一个事件我们得到的信息量大小。所以数学上，信息熵其实是信息量的期望。熵的本质就是获得信息的代价，信息熵的公式<script type="math/tex">H(X)=-\Sigma P(X_i)log_2P(X_i)，其中X_i表示事物状态，P为概率</script></p>
<p>基尼指数表示从数据集中随机抽取两个样本，其类别标记不一致的概率，即选取划分后基尼指数最小的属性作为最优划分属性，公式为 <script type="math/tex">Gini(D)=1-\Sigma {P_k}^2</script></p>
<h3 id="决策树生成的算法"><a href="#决策树生成的算法" class="headerlink" title="决策树生成的算法"></a>决策树生成的算法</h3><p>ID3算法的核心是在决策树各个节点上应用信息增益准则选择特征，递归地构建决策树。<br>C4.5算法使用增益率（信息增益/信息熵）来选择特征，可以处理特征属性值连续的情况<br>CART算法使用基尼指数选择特征，且只考虑二分类情况，生成二叉树，包含后剪枝操作</p>
<h3 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h3><p>二分递归分割，每个非叶子节点都有两个分支，左分支为True，有分支为False。</p>
<h4 id="cart回归树"><a href="#cart回归树" class="headerlink" title="cart回归树"></a>cart回归树</h4><p>用平方误差最小化准则，选择最优切分变量与切分点（误差平方和），产生输出值，依据输出值和样本量的残差，迭代选取变量和切分点，直至回归方程的最终误差平方和满足条件</p>
<h4 id="cart分类树"><a href="#cart分类树" class="headerlink" title="cart分类树"></a>cart分类树</h4><p>用基尼指数最小化准则</p>
<h4 id="连续值处理（C4-5算法）"><a href="#连续值处理（C4-5算法）" class="headerlink" title="连续值处理（C4.5算法）"></a>连续值处理（C4.5算法）</h4><p>在连续型变量中，找到划分点，且该划分点的信息增益值最大；</p>
<h4 id="缺失值处理（C4-5算法）"><a href="#缺失值处理（C4-5算法）" class="headerlink" title="缺失值处理（C4.5算法）"></a>缺失值处理（C4.5算法）</h4><p>考虑无缺失样本下的各个特征的信息增益，以该特征下无缺失样本所占比例为权重，产生新的信息增益</p>
<h3 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h3><p>为防止过拟合，需要对决策树进行简化，降低复杂度<br>预剪枝：当决策树在生成时当达到该指标时就停止生长，比如小于一定的信息获取量或是一定的深度，就停止生长。<br>后剪枝：当决策树生成完后，再进行剪枝操作。即将子树替换为叶节点</p>
<h3 id="模型特点-1"><a href="#模型特点-1" class="headerlink" title="模型特点"></a>模型特点</h3><p>能够处理数值和非数值属性<br>对数值属性的极端值不敏感<br>对属性的线性相关性不敏感<br>可以自动处理缺失值<br>很高的可解释性<br>可以用于多分类场景<br>不支持增量训练<br>在属性选择、属性分裂时，无法纳入属性间的交互性</p>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p>随机森林属于集成模型的一种。它由若干棵决策树构成，最终的判断结果由每一棵决策树的结果进行简单投票决定。</p>
<h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><p>通过构建并结合多个学习器来完成学习任务，不同类型的个体学习器，这样的集成是异质的。集成学习的好坏与个体学习器的准确性和多样性相关</p>
<h3 id="Boosting（AdaBoost算法）"><a href="#Boosting（AdaBoost算法）" class="headerlink" title="Boosting（AdaBoost算法）"></a>Boosting（AdaBoost算法）</h3><p>基于前一个个体学习器对训练样本的表现，对训练样本进行调整（降低正确样本权重，提高错误样本权重），产生新的学习器，直至到达指定个数的基学习器数目。根据这些基学习器的效果，进行权重分布，线性组合。</p>
<p>使用最广泛的Adaboost弱学习器是决策树和神经网络。对于决策树，Adaboost分类用了CART分类树，而Adaboost回归用了CART回归树。<br>Adaboost算法为加法模型，损失函数为指数函数，学习算法为前向分步的二分类方法。</p>
<p>提升树(boosting decision tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Decision Tree) ，提升树算法中弱学习器限定了只能使用CART回归树模型。</p>
<h3 id="Bagging和随机森林"><a href="#Bagging和随机森林" class="headerlink" title="Bagging和随机森林"></a>Bagging和随机森林</h3><p>Bagging。基于样本划分方法中的自助法(bootstrapping)，即对于含m个样本的数据集，开始有放回的抽取m次,作为训练集,然后把数据集和训练集的差集作为测试集，其中样本不被抽到的概率为36.8%，被重复抽到的概率为63.2%。在对预测输出进行结合时，通常对分类任务使用简单投票法，对回归任务使用简单平均法。</p>
<p>随机森林(Random Forest)<br>以决策树为基学习器构建，对样本选取基于Bagging方法，对属性的选取采取从属性集合中无放回地抽取一部分属性进行决策树模型的开发，对整体的d个属性，一般选取其中的k个属性，满足<script type="math/tex">k=log_2d</script></p>
<h3 id="Boosting-和-Bagging-区别"><a href="#Boosting-和-Bagging-区别" class="headerlink" title="Boosting 和 Bagging 区别"></a>Boosting 和 Bagging 区别</h3><p>Boosting的弱学习器存在强依赖关系，必须串行生成， Boosting主要关注降低偏差，可基于泛化性能弱的学习器构建出强的集成<br>Bagging的弱学习器之间没有依赖关系，可以并行生成</p>
<h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>GBDT使用了前向分布算法，在函数空间中利用梯度下降法进行优化。常见的有回归提升树、二元分类、多元分类三个GBDT算法，Freidman提出的梯度提升算法：利用最速下降的近似方法，即利用损失函数的负梯度在当前模型的值，作为回归问题中提升树算法的残差的近似值</p>
<h3 id="GBDT模型"><a href="#GBDT模型" class="headerlink" title="GBDT模型"></a>GBDT模型</h3><p>强分类器由多个弱分类器线性相加，h为cart树，a为cart树参数，GBDT中每颗树学的是之前所有树结论和的残差<br>$ f(x;[\beta_t,a_t])=\sum_{t=1}^T \beta_t h(x;a_t) $</p>
<p>第t步的模型可以写成：<br>$ f_t(x)=f_{t-1}(x)+\beta_th_t(x;a_t) $</p>
<p>构建h(x),使第t轮的损失函数最小：<br>$ L(y,f_t(x))=L(y,f_{t-1}(x)+\beta_th_t(x)) $</p>
<h3 id="GBDT-梯度拟合"><a href="#GBDT-梯度拟合" class="headerlink" title="GBDT 梯度拟合"></a>GBDT 梯度拟合</h3><p>对于损失函数$ L(y,f_t(x)) ,需求出最优解f^*(x)$</p>
<p>$ \rho_t=argminL(y,f_{t-1}(x)-\rho \cfrac {\partial L(y,f(x_{t-1}))}{\partial f(x_{t-1})} ) $<br>$ (\beta_t,a_t)=argminL(y,f_{t-1}(x)+\beta_th_t(x;a_t)) $<br>$ a_t=argmin \sum(-\cfrac {\partial L(y_i,f(x_i))}{\partial f(x_i)}-\beta_ih_i(x;a_i))^2 $</p>
<h3 id="GBDT常用损失函数"><a href="#GBDT常用损失函数" class="headerlink" title="GBDT常用损失函数"></a>GBDT常用损失函数</h3><p>指数损失函数 $  L(y,f(x))=exp(-yf(x)) $</p>
<p>负二项对数函数 $ L(y,f(x))=log(1+exp(-2yf(x))),y \in (-1,1) $ 用于二元分类场景<br>平方损失函数 $ L(y,f(x))=(y-f(x))^2 $ 用于回归场景</p>
<h2 id="Xgboost"><a href="#Xgboost" class="headerlink" title="Xgboost"></a>Xgboost</h2><p>Xgboost与GBDT，两者都是boosting方法，不同在于，GBDT利用梯度下降法进行优化，XGboost利用牛顿法进行优化<br>目标函数为<br>$ L(y,y_{t-1}(x)+f_t(x))+\sum \Omega(f_t) $<br>对于每颗树，由结构部分q和叶子权重部分组成w定义。<br>$ f_t(x)=w_q(x) $<br>后者为正则化项，正则化项与树的叶子节点的数量T和叶子节点的值w有关,$\Omega(f_t)=\gamma T+\cfrac{1}{2}\lambda||w||^2  $</p>
<p>用泰勒展开<br>$ f(x+\Delta x)=f(x)+f’(x)\Delta x+\cfrac{f^{(2)}(x)}{2}\Delta x^2 $<br>定义<br>$ g_i=\partial L(y,y_{t-1}(x)),h_i=\partial^2 L(y,y_{t-1}(x)) $<br>则有<br>$ Obj_i= \sum (L(y,y_{t-1})+g_if_t(x_i)+\cfrac{1}{2}h_if^2_t(x_i))+\Omega(f_t)   ​$</p>
<h2 id="类别不平衡"><a href="#类别不平衡" class="headerlink" title="类别不平衡"></a>类别不平衡</h2><p>训练样本中不同类别的样本数目差距过大的情况（正例过少，反例过多），实行一个基本策略，“再缩放”</p>
<ul>
<li>欠采样（undersampling）</li>
</ul>
<p>去除一些反例，使正、反例数目接近.代表算法EasyEnsemble</p>
<ul>
<li>过采样（oversampling）</li>
</ul>
<p>增加一些正例，使得双方数目接近。代表算法SMOTE</p>
<ul>
<li>阈值移动</li>
</ul>
<p>直接基于原始训练集进行学习，在用分类器进行预测时，<script type="math/tex">\cfrac{y}{1-y}×\cfrac{m^-}{m^+} 作为阈值(m为观测数目)</script></p>
<p><strong>SMOTE算法</strong></p>
<p>随机过采样采取简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题。SMOTE算法的基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集</p>
<p>算法流程:<br>1.对于少数类中每一个样本x，以欧氏距离为标准计算它到少数类样本集中所有样本的距离，得到其k近邻。<br>2.根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本x，从其k近邻中随机选择若干个样本，假设选择的近邻为$x_i$。<br>3.对于每一个随机选出的近邻$x_i$，分别与原样本按照公式$ x_{new}=x+rand(0,1)*|x-x_i| $构建新的样本 </p>
<h2 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h2><h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><p>PCA的思想是将n维特征映射到k维上，这k维是重新构造出来的k维特征，且k维应线性无关，而不是简单地从n维特征中去除其余n-k维特征。即线代知识中通过矩阵的变化，将一个维度中的向量变化到另一个维度中，实现降维的效果。</p>
<p>维度的协方差矩阵中，对角线是各维度的样本方差，非对角线位置是不同维度的协方差，也即两个维度的相关性。而降维后理想化即是非对角线位置各维度的相关性为0，协方差矩阵为对角矩阵。转为矩阵运算原理，降维的过程也即是将协方差矩阵经过变化为对角矩阵。</p>
<p>$  M^{-1}PM=\Lambda $ ,其中M为矩阵P变化的特征向量，也即正交矩阵，$\Lambda$ 为对角矩阵，对角元素为特征值$\lambda。​$<br>M也为另一坐标系下的基向量，当坐标系经过矩阵P变化后，获得的标准基向量坐标系的变化 </p>
<h3 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h3><p>将任意矩阵分解<br>$ A_{m×n} =U_{m×m} \sum_{m×n} V_{n×n}$，其中U的列向量为$AA^T$的特征向量，称为A的左奇异向量，V的列向量为$A^TA$的特征向量，称为A的右奇异向量，$\sum$ 为主对角线为奇异值$\sigma$，非主对角线位置为0的矩阵 ,其中$\sigma$ 也为$A^TA$的特征值的平方根<br> 选取奇异值最大的前k列，重新构造来代替原矩阵A，实现降维效果</p>
<h3 id="t-SNE"><a href="#t-SNE" class="headerlink" title="t-SNE"></a>t-SNE</h3><p>t-SNE将样本点间的相似度关系转化为概率<br> 在原空间（高维空间）中转化为基于高斯分布的概率；在嵌入空间（低维空间）中转化为基于t分布的概率。这使得t-SNE不仅可以关注局部（SNE只关注相邻点之间的相似度映射而忽略了全局之间的相似度映射，使得可视化后的边界不明显），还关注全局，使可视化效果更好（簇内不会过于集中，簇间边界明显）</p>
<h2 id="向量机"><a href="#向量机" class="headerlink" title="向量机"></a>向量机</h2><p>SVM（Support Vector Machine）的模型是让所有点到超平面的距离大于一定的距离，也就是所有的分类点要在各自类别的支持向量两边。</p>
<p>支持向量：和超平面平行的保持一定的函数距离的这两个超平面对应的向量，两个异类支持向量到超平面的距离之和称为间隔<script type="math/tex">\gamma=\cfrac{2}{||w||}，其中||w||是w的L2范数</script></p>
<blockquote>
<p>SVM模型目标函数,即最大化间隔：</p>
<script type="math/tex; mode=display">min \cfrac{1}{2}||w||^2</script><script type="math/tex; mode=display">s.t. y_i(w^Tx_i+b) \geq 1</script></blockquote>
<p>-</p>
<blockquote>
<p>对线性不可分数据集，目标函数为软间隔最大化，即对每个样本点引进松弛变量 <script type="math/tex">\delta \geq0</script></p>
<script type="math/tex; mode=display">min \cfrac{1}{2}||w||^2+C\sum \delta_i</script><script type="math/tex; mode=display">s.t. y_i(w^Tx_i+b) \geq 1-\delta_i</script></blockquote>
<p>-</p>
<blockquote>
<p>对非线性问题，先将其转化为线性问题。即将样本通过<script type="math/tex">\phi 函数</script>映射到更高维的特征空间，使其线性可分。核函数</p>
<script type="math/tex; mode=display">K(x_i,x_j)=\phi(x_i)^T\phi(x_j)  $$。将$$x_i 和 x_j$$ 在特征空间的内积等于它们在原始空间中通过核函数K实现
$$ min \cfrac{1}{2}||w||^2</script><script type="math/tex; mode=display">s.t. y_i(w^T\phi(x_i)+b) \geq 1</script></blockquote>
<p>-</p>
<blockquote>
<p>核函数<br>线性核函数 <script type="math/tex">K(x,z)=x^Tz</script><br>多项式核函数 <script type="math/tex">k(x,z)=(x^Tz)^d</script><br>高斯核函数<script type="math/tex">K(x,z)=exp(-\cfrac{||x-z||^2}{2\sigma ^2})</script><br>Sigmoid核函数<script type="math/tex">K(x,z)=tanh(\beta x^Tz+\theta)</script></p>
</blockquote>
<h2 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h2><p>最大熵原理：在所有可能的概率模型中，条件熵最大的模型是最好的模型</p>
<script type="math/tex; mode=display">H(X)=-\Sigma P(X_i)log_2P(X_i)，其中X_i表示事物状态，P为概率</script><p>条件熵关于条件概率分布<script type="math/tex">H(Y|X)=-\Sigma P(X_i)P(Y_i|X
_i)log_2P(|Y_i|X_i)</script></p>
<h2 id="异常点检测"><a href="#异常点检测" class="headerlink" title="异常点检测"></a>异常点检测</h2><p>异常点检测算法，基于专门的异常点检测算法来做。算法的代表是One Class SVM和Isolation Forest.</p>
<h3 id="One-Class-SVM"><a href="#One-Class-SVM" class="headerlink" title="One Class SVM"></a>One Class SVM</h3><p>应用场景：样本中，只有一种类型的样本，或有两种类型样本，但其中一类型样本数目远少于另一类型样本数目。属于无监督学习。<br>对于只有一个类的情况，采用SVDD（support vector domain description），它的基本思想是，训练出一个最小的超球面（超球面是指3维以上的空间中的球面，对应的2维空间中就是曲线，3维空间中就是球面，3维以上的称为超球面），把这堆数据全都包起来，识别一个新的数据点时，如果这个数据点落在超球面内，就是这个类，否则不是。优化目标是求出能够包含正常数据样本的最小超球体的中心a和半径R</p>
<h3 id="Isolation-Forest"><a href="#Isolation-Forest" class="headerlink" title="Isolation Forest"></a>Isolation Forest</h3><p>iTree是一种随机二叉树，且随机选择划分属性和划分点（值）。iTree能有效检测异常的假设是：异常点一般都是非常稀有的，在iTree中会很快被划分到叶子节点，因此可以用叶子节点到根节点的路径h(x)长度来判断一条记录x是否是异常点。<br>s(x,n) 表明x在由n个样本的训练数据构成的iTree的异常指数，越接近1，异常可能性越高，</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com2019/02/05/Python基础/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Marcus Anthony">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/mar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Marcus Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2019/02/05/Python基础/" itemprop="url">Python基础</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-02-05T18:51:15+08:00">
                2019-02-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><p>list是一种有序的集合，可以随时添加和删除其中的元素<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">list =[] </span><br><span class="line"># 空列表</span><br><span class="line"></span><br><span class="line">list.append(&apos;Google&apos;)</span><br><span class="line"># 添加元素，对列表自己内部进行操作， 不会有返回值</span><br><span class="line"></span><br><span class="line">list.extend(seq)</span><br><span class="line"># 列表末尾一次性追加另一个序列中的多个值,原列表内部操作，不会有返回值</span><br><span class="line"></span><br><span class="line">del list[2]</span><br><span class="line"># 删除元素，对列表自己内部进行操作， 不会有返回值</span><br><span class="line"></span><br><span class="line">del list[:]</span><br><span class="line"># 删除所有元素</span><br><span class="line"></span><br><span class="line">list.count(obj)</span><br><span class="line"># 统计某个元素在列表中出现的次数</span><br><span class="line"></span><br><span class="line">list.index(obj)</span><br><span class="line"># 从列表中找出某个值第一个匹配项的索引位置</span><br><span class="line"></span><br><span class="line">list.insert(index, obj)</span><br><span class="line"># 将对象插入列表,对列表自己内部进行操作， 不会有返回值</span><br><span class="line"></span><br><span class="line">list.pop([index=-1])</span><br><span class="line"># 移除列表中的一个元素（默认最后一个元素），并且返回该元素值</span><br><span class="line"></span><br><span class="line">list.remove(obj)</span><br><span class="line">#移除列表中某个值的第一个匹配项,对列表自己内部进行操作，不会有返回值</span><br><span class="line"></span><br><span class="line">list.reverse()</span><br><span class="line"># 反向列表中元素,不会有返回值</span><br><span class="line"></span><br><span class="line">list.sort(cmp=None, key=None, reverse=False)</span><br><span class="line"># 对原列表进行排序,不会有返回值</span><br><span class="line"></span><br><span class="line">sorted(list,key=,reverse=)</span><br><span class="line"># 不改变原列表，返回新列表</span><br></pre></td></tr></table></figure></p>
<h3 id="IO-模块"><a href="#IO-模块" class="headerlink" title="IO 模块"></a>IO 模块</h3><p>open()函数用于打开一个文件，创建一个file对象  ,通过file对象，可以得到有关该文件的各种信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">file=open(filename,&apos;r+&apos;)</span><br><span class="line"># 打开文件用于读写</span><br><span class="line"></span><br><span class="line">file=open(filename,&apos;w+&apos;)</span><br><span class="line"># 打开文件用于读写，若文件存在则覆盖，若不存在，则创建文件</span><br><span class="line"></span><br><span class="line">file=open(filename,&apos;a+&apos;)</span><br><span class="line"># 打开文件用于读写，若文件存在，文件末尾追加，若不存在，则创建文件</span><br><span class="line"></span><br><span class="line">file_read=file.read([count])</span><br><span class="line"># 从文件的开头开始读入,count为字节数</span><br><span class="line"></span><br><span class="line">file_read=file.readline([size])</span><br><span class="line"># 返回包含size行的列表,size 未指定则返回全部行</span><br><span class="line"></span><br><span class="line">file_write=file.write(string)</span><br><span class="line"># write()方法不会在字符串的结尾添加换行符(&apos;\n&apos;)</span><br><span class="line"></span><br><span class="line">file=file.close()</span><br><span class="line"># 关闭文件</span><br><span class="line"></span><br><span class="line">for line in file:</span><br><span class="line"># file对象可迭代</span><br><span class="line"></span><br><span class="line">with open(filename) as f:</span><br><span class="line"># with语句可自动调用file的close()方法</span><br></pre></td></tr></table></figure>
<h3 id="OS模块"><a href="#OS模块" class="headerlink" title="OS模块"></a>OS模块</h3><p>Python 的 os 模块封装了常见的文件和目录操作</p>
<blockquote>
<p>os: This module provides a portable way of using operating system dependent functionality.<br>这个模块提供了一种方便的使用操作系统函数的方法</p>
<p>sys: This module provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter.<br>这个模块可供访问由解释器使用或维护的变量和与解释器进行交互的函数。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>os.mkdir</td>
<td>创建目录</td>
</tr>
<tr>
<td>os.rmdir</td>
<td>删除目录</td>
</tr>
<tr>
<td>os.chdir(path)</td>
<td>更改目录</td>
</tr>
<tr>
<td>os.rename</td>
<td>重命名</td>
</tr>
<tr>
<td>os.remove</td>
<td>删除文件</td>
</tr>
<tr>
<td>os.getcwd</td>
<td>获取当前工作路径</td>
</tr>
<tr>
<td>os.walk</td>
<td>遍历目录</td>
</tr>
<tr>
<td>os.path.join</td>
<td>连接目录与文件名</td>
</tr>
<tr>
<td>os.path.split</td>
<td>分割文件名与目录</td>
</tr>
<tr>
<td>os.path.abspath</td>
<td>获取绝对路径</td>
</tr>
<tr>
<td>os.path.dirname</td>
<td>获取路径</td>
</tr>
<tr>
<td>os.path.basename</td>
<td>获取文件名或文件夹名</td>
</tr>
<tr>
<td>os.path.splitext</td>
<td>分离文件名与扩展名</td>
</tr>
<tr>
<td>os.path.isfile</td>
<td>判断给出的路径是否是一个文件</td>
</tr>
<tr>
<td>os.path.isdir</td>
<td>判断给出的路径是否是一个目录</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">方法</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">sys.argv</td>
<td style="text-align:center">命令行参数List，第一个元素是程序本身路径</td>
</tr>
<tr>
<td style="text-align:center">sys.modules.keys()</td>
<td style="text-align:center">返回所有已经导入的模块列表</td>
</tr>
<tr>
<td style="text-align:center">sys.exc_info()</td>
<td style="text-align:center">获取当前正在处理的异常类,exc_type、exc_value、exc_traceback当前处理的异常详细信息</td>
</tr>
<tr>
<td style="text-align:center">sys.exit(n)</td>
<td style="text-align:center">退出程序，正常退出时exit(0)</td>
</tr>
<tr>
<td style="text-align:center">sys.hexversion</td>
<td style="text-align:center">获取Python解释程序的版本值，16进制格式如：0x020403F0</td>
</tr>
<tr>
<td style="text-align:center">sys.version</td>
<td style="text-align:center">获取Python解释程序的版本信息</td>
</tr>
<tr>
<td style="text-align:center">sys.maxint</td>
<td style="text-align:center">最大的Int值</td>
</tr>
<tr>
<td style="text-align:center">sys.maxunicode</td>
<td style="text-align:center">最大的Unicode值</td>
</tr>
<tr>
<td style="text-align:center">sys.modules</td>
<td style="text-align:center">返回系统导入的模块字段，key是模块名，value是模块</td>
</tr>
<tr>
<td style="text-align:center">sys.path</td>
<td style="text-align:center">返回模块的搜索路径，初始化时使用PYTHONPATH环境变量的值</td>
</tr>
<tr>
<td style="text-align:center">sys.platform</td>
<td style="text-align:center">返回操作系统平台名称</td>
</tr>
<tr>
<td style="text-align:center">sys.stdout</td>
<td style="text-align:center">标准输出</td>
</tr>
<tr>
<td style="text-align:center">sys.stdin</td>
<td style="text-align:center">标准输入</td>
</tr>
<tr>
<td style="text-align:center">sys.stderr</td>
<td style="text-align:center">错误输出</td>
</tr>
<tr>
<td style="text-align:center">sys.exc_clear()</td>
<td style="text-align:center">用来清除当前线程所出现的当前的或最近的错误信息</td>
</tr>
<tr>
<td style="text-align:center">sys.exec_prefix</td>
<td style="text-align:center">返回平台独立的python文件安装的位置</td>
</tr>
<tr>
<td style="text-align:center">sys.byteorder</td>
<td style="text-align:center">本地字节规则的指示器，big-endian平台的值是’big’,little-endian平台的值是’little’</td>
</tr>
<tr>
<td style="text-align:center">sys.copyright</td>
<td style="text-align:center">记录python版权相关的东西</td>
</tr>
<tr>
<td style="text-align:center">sys.api_version</td>
<td style="text-align:center">解释器的C的API版本</td>
</tr>
</tbody>
</table>
</div>
<h3 id="re模块​"><a href="#re模块​" class="headerlink" title="re模块​"></a>re模块​</h3><h4 id="re-match-pattern-string-flags"><a href="#re-match-pattern-string-flags" class="headerlink" title="re.match(pattern, string [, flags] )"></a>re.match(pattern, string [, flags] )</h4><p> 需完全匹配出表达式，才成功，从string头开始到pattern匹配结束，同时匹配终止，不再匹配后面字符</p>
<blockquote>
<p>import re<br>pattern= re.compile(r’hello’)<br>result1=re.match(pattern,’hello’)<br>print result1.group()</p>
</blockquote>
<p> match对象的属性</p>
<ul>
<li>sring：匹配时使用的文本</li>
<li>re：匹配时使用的正则对象</li>
<li>group() :返回分组截获的字符串</li>
<li>groups()：以元组形式返回全部分组截获的字符串</li>
<li>groupdict():返回有别名的组，字典形式展现</li>
<li>expand(template) 将匹配到的分组代入template中然后返回，template可以用 \id或\g 引用分组</li>
</ul>
<h4 id="re-search-pattern-string-flags"><a href="#re-search-pattern-string-flags" class="headerlink" title="re.search(pattern, string [, flags])"></a>re.search(pattern, string [, flags])</h4><p> search方法与match方法区别：在于match() 函数从string开始位置匹配，search() 会扫描整个string查找匹配</p>
<h4 id="re-split-pattern-string-maxsplit"><a href="#re-split-pattern-string-maxsplit" class="headerlink" title="re.split(pattern, string [,maxsplit])"></a>re.split(pattern, string [,maxsplit])</h4><p>按照能够匹配的子串将string分割后返回列表。maxsplit用于指定最大分割次数，不指定将全部分割   </p>
<blockquote>
<p>import re<br>pattern= re.compile(r’\d+’)<br>print re.split(pattern,’one1two2three3four4’)<br>[‘one’, ‘two’, ‘three’, ‘four’, ‘’]    </p>
</blockquote>
<h4 id="re-findall-pattern-string-flags"><a href="#re-findall-pattern-string-flags" class="headerlink" title="re.findall(pattern, string[, flags])"></a>re.findall(pattern, string[, flags])</h4><p>搜索string，以列表形式返回全部能匹配的子串</p>
<blockquote>
<p>import re<br>pattern = re.compile(r’\d+’)<br>print re.findall(pattern,’one1two2three3four4’)<br>[‘1’, ‘2’, ‘3’, ‘4’]</p>
</blockquote>
<h4 id="re-finditer-pattern-string-flags"><a href="#re-finditer-pattern-string-flags" class="headerlink" title="re.finditer(pattern, string[, flags])"></a>re.finditer(pattern, string[, flags])</h4><p>搜索string，返回一个顺序访问每一个匹配结果（Match对象）的迭代器</p>
<blockquote>
<p>import re<br>pattern = re.compile(r’\d+’)<br>for m in re.finditer(pattern,’one1two2three3four4’):<br>print m.group()</p>
</blockquote>
<h4 id="re-sub-pattern-repl-string-count"><a href="#re-sub-pattern-repl-string-count" class="headerlink" title="re.sub(pattern, repl, string[, count])"></a>re.sub(pattern, repl, string[, count])</h4><p>使用repl替换string中每一个匹配的子串后返回替换后的字符串</p>
<h4 id="re-subn-pattern-repl-string-count"><a href="#re-subn-pattern-repl-string-count" class="headerlink" title="re.subn(pattern, repl, string[, count])"></a>re.subn(pattern, repl, string[, count])</h4><p>返回 (sub(repl, string[, count]), 替换次数)</p>
<h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><ul>
<li><p>execfile()函数：用来执行一个文件</p>
</li>
<li><p>file()函数：用来创建一个file对象</p>
</li>
<li><p>filter(function, iterable) 函数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;daily_spend = [110.32, 0, 445.32, 0, 88.83,0]</span><br><span class="line">&gt;&gt;&gt;has_spend = filter(lambda x: x!=0,daily_spend)</span><br><span class="line">&gt;&gt;&gt;print(list(has_spend))</span><br><span class="line">[110.32, 445.32, 88.83]</span><br></pre></td></tr></table></figure>
</li>
<li><p>iter()函数：用来生成迭代器</p>
</li>
<li><p>map(function, iterable, …)函数会根据提供的函数对指定序列做映射</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; map(lambda x: x+1,[1,2,3,4]) </span><br><span class="line">[2, 3, 4, 5]  </span><br><span class="line">&gt;&gt;&gt; map(lambda x,y: x+y,[1,2,3,4],(10,20,30,40))</span><br><span class="line">[11, 22, 33, 44]</span><br></pre></td></tr></table></figure>
<ul>
<li>reduce(function, iterable[, initializer]) 函数会对参数序列中元素进行累积</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;daily_spend = [110.32, 0, 445.32, 0, 88.83,0]</span><br><span class="line">&gt;&gt;&gt;print(reduce(lambda x,y:x+y, daily_spend))</span><br><span class="line">644.47</span><br></pre></td></tr></table></figure>
<ul>
<li>zip函数：将多个序列相同位置上的元素，组合成元组后放入列表中</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;days, spend = [10,25,30], [12.20, 20.43, 44.32]</span><br><span class="line">&gt;&gt;&gt;consumptions = zip(days, spend)</span><br><span class="line">&gt;&gt;&gt;print(list(consumptions ))</span><br><span class="line">[(10, 12.2), (25, 20.43), (30, 44.32)]</span><br></pre></td></tr></table></figure>
<ul>
<li>sort()函数应用在list上，对列表自己内部进行排序， 不会有返回值， 因此返回为None，sorted()函数应用在所有可迭代对象，产生新的对象</li>
<li>set() 函数创建一个无序不重复元素集，可进行关系测试</li>
<li>slice() 函数实现切片对象</li>
<li>strip() 方法用于移除字符串头尾指定的字符（默认删除所有空白符）</li>
<li><p>Python内置的enumerate函数可以把一个list变成索引-元素对</p>
</li>
<li><p>生成器：generator，是种一边循环一边计算的机制，generator保存的是算法，可以通过next()函数获得generator的下一个返回值</p>
</li>
<li>迭代器：Iterator表示的是一个数据流，对象可以被next()函数调用并不断返回下一个数据，可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据</li>
<li><p>yield函数，在调用生成器运行的过程中，每次遇到 yield 时函数会暂停并保存当前所有的运行信息，返回 yield 的值, 并在下一次执行 next() 方法时从当前位置继续运行。</p>
</li>
<li><p>format 格式化函数</p>
</li>
<li>‘sep’.join() 返回一个以分隔符sep连接各个元素后生成的字符串</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&quot;&#123;&#125; &#123;&#125;&quot;.format(&quot;hello&quot;, &quot;world&quot;)    # 不设置指定位置，按默认顺序</span><br><span class="line">&apos;hello world&apos;</span><br><span class="line"></span><br><span class="line"> &quot;&#123;0&#125; &#123;1&#125;&quot;.format(&quot;hello&quot;, &quot;world&quot;)  # 设置指定位置</span><br><span class="line">&apos;hello world&apos;</span><br><span class="line"></span><br><span class="line"> &quot;&#123;1&#125; &#123;0&#125; &#123;1&#125;&quot;.format(&quot;hello&quot;, &quot;world&quot;)  # 设置指定位置</span><br><span class="line">&apos;world hello world&apos;</span><br><span class="line"></span><br><span class="line"> site = &#123;&quot;name&quot;: &quot;菜鸟教程&quot;, &quot;url&quot;: &quot;www.runoob.com&quot;&#125;</span><br><span class="line"> print(&quot;网站名：&#123;name&#125;, 地址 &#123;url&#125;&quot;.format(**site))</span><br><span class="line"></span><br><span class="line"> my_list = [&apos;菜鸟教程&apos;, &apos;www.runoob.com&apos;]</span><br><span class="line"> print(&quot;网站名：&#123;0[0]&#125;, 地址 &#123;0[1]&#125;&quot;.format(my_list))  # &quot;0&quot; 是可选的</span><br></pre></td></tr></table></figure>
<h3 id="Notion事项"><a href="#Notion事项" class="headerlink" title="Notion事项"></a>Notion事项</h3><ul>
<li>多行注释使用三个单引号(‘’’)或三个双引号(“””)    </li>
<li>元组用”()”标识。内部元素用逗号隔开。但是元组不能二次赋值，相当于只读列表,元组中的元素值是不允许删除的，但我们可以使用del语句来删除整个元组     </li>
<li>continue 用于跳过该次循环，break 则是用于退出循环    </li>
<li>pass是空语句，是为了保持程序结构的完整性,pass 不做任何事情，一般用做占位语句   </li>
<li>Python不支持单字符类型，单字符也在Python也是作为一个字符串使用    </li>
<li>r/R    原始字符串,所有的字符串都是直接按照字面的意思来使用，没有转义特殊或不能打印的字符。 原始字符串除在字符串的第一个引号前加上字母”r”（可以大小写）以外，与普通字符串有着几乎完全相同的语法    </li>
<li>在 python 中，strings, tuples, 和 numbers 是不可更改的对象，而 list,dict 等则是可以修改的对象 </li>
<li>set和dict的唯一区别仅在于没有存储对应的value</li>
<li>seek（offset [,from]）方法改变当前文件的位置。Offset变量表示要移动的字节数。From变量指定开始移动字节的参考位置。<br>如果from被设为0，这意味着将文件的开头作为移动字节的参考位置。如果设为1，则使用当前的位置作为参考位置。如果它被设为2，那么该文件的末尾将作为参考位置</li>
<li>input() 和 raw_input() 这两个函数均能接收 字符串 ，但 raw_input() 直接读取控制台的输入（任何类型的输入它都可以接收）。而对于 input() ，它希望能够读取一个合法的 python 表达式，即你输入字符串的时候必须使用引号将它括起来</li>
<li>tuple所谓的“不变”是说，tuple的每个元素，指向永远不变。list的元素可变</li>
<li>函数返回值，可以用return语句返回，函数体内部的语句在执行时，一旦执行到return时，函数就执行完毕，并将结果返回，函数执行完毕也没有return语句时，自动return None</li>
<li><p>Python的函数返回多值其实就是返回一个tuple</p>
</li>
<li><p>不可变类型参数传递：整数、字符串、元组。</p>
<p>如fun（a），传递的只是a的值，没有影响a对象本身。比如在 fun（a）内部修改 a 的值，只是修改另一个复制的对象，不会影响 a 本身。</p>
</li>
<li><p>可变类型参数传递：如列表，字典。</p>
<p>如 fun（la），则是将 la 真正的传过去，修改后fun外部的la也会受影响</p>
</li>
<li><p>定义可变参数。可变参数就是传入的参数个数是可变的，仅仅在参数前面加了一个*号</p>
<p>可变参数允许你传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。而关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict</p>
</li>
<li><p>*args是可变参数，args接收的是一个tuple</p>
</li>
<li><p>**kw是关键字参数，kw接收的是一个dict</p>
</li>
<li><p>可变参数既可以直接传入：func(1, 2, 3)，又可以先组装list或tuple，再通过<em>args传入：func(</em>(1, 2, 3))</p>
</li>
<li><p>关键字参数既可以直接传入：func(a=1, b=2)，又可以先组装dict，再通过<em> </em> kw传入：func(**{‘a’: 1, ‘b’: 2})</p>
</li>
<li><p>实例的变量名如果以__开头（两个下划线），就变成了一个私有变量（private），只有内部可以访问，外部不能访问</p>
</li>
<li><p><code>__slots__</code>变量，可限制该class实例能添加的属性</p>
</li>
<li><p><code>__init__</code>方法的第一个参数永远是self，表示创建的实例本身，因此，在<code>__init__</code>方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身</p>
</li>
<li><p>imp.load_source(‘module_name’,’/path/file.py’)<br>将file.py 文件导入到 module_name中，module_name 可以自定义</p>
</li>
<li><p>每个Python脚本在运行时都有一个“name”属性。如果脚本作为模块被导入，则其“name”属性的值被自动设置为模块名；如果脚本独立运行，则其“name”属性值被自动设置为“main”</p>
</li>
<li><p>python查找变量是顺序是：先局部变量，再全局变量。</p>
</li>
<li><p>直接赋值：其实就是对象的引用（别名），即变量指向同一对象</p>
</li>
<li><p>浅拷贝(copy)：拷贝父对象，不会拷贝对象的内部的子对象，即内部子对象做同步变化</p>
</li>
<li><p>深拷贝(deepcopy)： copy 模块的 deepcopy 方法，完全拷贝了父对象及其子对象。</p>
</li>
<li><p>super() 函数是用于调用父类(超类)的一个方法</p>
</li>
<li><p>isinstance(a, list) 判断变量类型</p>
</li>
</ul>
<h3 id="itertools模块"><a href="#itertools模块" class="headerlink" title="itertools模块"></a>itertools模块</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"> count(start=0, step=1)</span><br><span class="line"># 创建一个迭代器，生成从n开始的连续整数</span><br><span class="line"></span><br><span class="line">cycle(iterable)</span><br><span class="line"># 创建一个能在一组值间无限循环的迭代器</span><br><span class="line"></span><br><span class="line">repeat(object[, times])</span><br><span class="line"># 创建一个迭代器，重复生成object，times（如果已提供）指定重复计数</span><br><span class="line"></span><br><span class="line">chain(*iterables)</span><br><span class="line"># 将多个迭代器作为参数, 但只返回单个迭代器, 它产生所有参数迭代器的内容, 就好像他们是来自于一个单一的序列</span><br><span class="line"></span><br><span class="line">compress(data, selectors)</span><br><span class="line"># 提供一个选择列表，对原始数据进行筛选</span><br><span class="line"></span><br><span class="line">dropwhile(predicate, iterable)</span><br><span class="line"># 创建一个迭代器，只要函数predicate(item)返回False，就会生成iterable中的项和所有后续项</span><br><span class="line"></span><br><span class="line">takewhile(predicate, iterable)</span><br><span class="line"># 创建一个迭代器，生成iterable中predicate(item)为True的项，只要predicate计算为False，迭代就会立即停止</span><br><span class="line"></span><br><span class="line">groupby(iterable[, key])</span><br><span class="line"># 返回一个产生按照key进行分组后的值集合的迭代器,生成元素(key, group)，其中key是分组的键值，group是迭代器，生成组成该组的所有项</span><br><span class="line"></span><br><span class="line">ifilter(predicate, iterable)</span><br><span class="line"># 返回当测试函数返回true时的项</span><br><span class="line"></span><br><span class="line">islice(iterable, start, stop[, step])</span><br><span class="line"># 如果省略了start，迭代将从0开始，如果省略了step，步幅将采用1.</span><br><span class="line"></span><br><span class="line">imap(function, *iterables)</span><br><span class="line"># 返回序列每个元素被func执行后返回值的序列的迭代器</span><br><span class="line"></span><br><span class="line">starmap(function, iterable)</span><br><span class="line"># 创建一个迭代器，生成值func(*item),其中item来自iterable</span><br><span class="line"></span><br><span class="line">tee(iterable[, n=2])</span><br><span class="line"># 从一个可迭代对象创建 n 个迭代器</span><br><span class="line"></span><br><span class="line">izip(*iterables)</span><br><span class="line"># 类似于内置函数zip(), 只是它返回的是一个迭代器而不是一个列表</span><br><span class="line"></span><br><span class="line">product(*iterables[, repeat])</span><br><span class="line"># 创建一个迭代器，生成表示item1，item2等中的项目的笛卡尔积的元组，repeat是一个关键字参数，指定重复生成序列的次数</span><br><span class="line"></span><br><span class="line">permutations(iterable[, r])</span><br><span class="line"># 返回iterable中所有长度为r的项目序列，如果省略了r，那么序列的长度与iterable中的项目数量相同</span><br><span class="line"></span><br><span class="line">combinations(iterable, r)</span><br><span class="line"># 创建一个迭代器，返回iterable中所有长度为r的子序列，返回的子序列中的项按输入iterable中的顺序排序</span><br><span class="line"></span><br><span class="line">combinations_with_replacement(iterable, r)</span><br><span class="line"># 与 combinations 非常类似。唯一的区别是，它会创建元素自己与自己的组合</span><br></pre></td></tr></table></figure>
<h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><p>字典的键一般是唯一的，如果重复最后的一个键值对会替换前面的，值不需要唯一；键必须不可变，所以可以用数字，字符串或元组充当，所以用列表就不行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">del dict[&apos;Name&apos;]</span><br><span class="line">## 删除键是&apos;Name&apos;的条目</span><br><span class="line"></span><br><span class="line">dict.clear()  </span><br><span class="line">## 清空词典所有条目</span><br><span class="line"></span><br><span class="line">del dict </span><br><span class="line">## 删除词典</span><br><span class="line"></span><br><span class="line">str(dict)</span><br><span class="line">## 返回string</span><br><span class="line"></span><br><span class="line">dict.copy()</span><br><span class="line">## 返回一个字典的浅复制</span><br><span class="line"></span><br><span class="line">dict.fromkeys(seq[, val])</span><br><span class="line">## 创建一个新字典，以序列 seq 中元素做字典的键，value 为字典所有键对应的初始值</span><br><span class="line"></span><br><span class="line">dict.get(key, default=None)</span><br><span class="line">## 返回指定键的值，如果值不在字典中返回默认值</span><br><span class="line"></span><br><span class="line">dict.has_key(key)</span><br><span class="line">## 如果键在字典dict里返回true，否则返回false</span><br><span class="line"></span><br><span class="line">dict.items()</span><br><span class="line">## 以列表返回可遍历的(键, 值) 元组数组</span><br><span class="line"></span><br><span class="line">dict.keys()</span><br><span class="line">## 以列表返回一个字典所有的键</span><br><span class="line"></span><br><span class="line">dict.values()</span><br><span class="line">## 以列表返回字典中的所有值</span><br><span class="line"></span><br><span class="line">dict.setdefault(key, default=None)</span><br><span class="line">## 如果字典中包含有给定键，则返回该键对应的值，否则返回为该键设置的值</span><br><span class="line"></span><br><span class="line">dict.update(dict2)</span><br><span class="line">## 把字典dict2的键/值对更新到dict里</span><br><span class="line">	</span><br><span class="line">pop(key[,default])</span><br><span class="line">## 删除字典给定键 key 所对应的值，返回值为被删除的值。key值必须给出。 否则，返回default值。</span><br><span class="line"></span><br><span class="line">popitem()</span><br><span class="line">## 随机返回并删除字典中的一对键和值。</span><br></pre></td></tr></table></figure>
<h3 id="正则"><a href="#正则" class="headerlink" title="正则"></a>正则</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">*</span><br><span class="line"># 零次或多次匹配前面的字符或子表达式</span><br><span class="line"></span><br><span class="line">+</span><br><span class="line"># 一次或多次匹配前面的字符或子表达式</span><br><span class="line"></span><br><span class="line">？</span><br><span class="line"># 零次或一次匹配前面的字符或子表达式</span><br><span class="line"></span><br><span class="line">.</span><br><span class="line"># 匹配除\r\n 之外的任何单个字符</span><br><span class="line"></span><br><span class="line">&#123;n&#125;</span><br><span class="line"># n是非负整数，正好匹配n次</span><br><span class="line"></span><br><span class="line">&#123;n,&#125;</span><br><span class="line"># 至少匹配n次</span><br><span class="line"></span><br><span class="line">\d </span><br><span class="line"># 数字[0-9]</span><br><span class="line"></span><br><span class="line">\D</span><br><span class="line"># 非数字</span><br><span class="line"></span><br><span class="line">\s</span><br><span class="line"># 空白字符</span><br><span class="line"></span><br><span class="line">\S</span><br><span class="line"># 非空白字符</span><br><span class="line"></span><br><span class="line">\w</span><br><span class="line"># 单词字符[A-Za-z0-9_]（字母、数字、下划线、汉字）</span><br><span class="line"></span><br><span class="line">\W</span><br><span class="line"># 非单词字符</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(?P&lt;name&gt;正则表达式) </span><br><span class="line">#命名分组，name是一个合法的标识符</span><br><span class="line"></span><br><span class="line">group(num)和groups()</span><br><span class="line">#获得分组内容 </span><br><span class="line"></span><br><span class="line">(?P=name)</span><br><span class="line"># 引入name分组匹配到的字符串</span><br><span class="line"></span><br><span class="line">\&lt;number&gt;</span><br><span class="line"># 引入编号为number的分组匹配到的字符串</span><br><span class="line"></span><br><span class="line">(?&lt;=正则表达式)</span><br><span class="line"># (?&lt;=\d)a   匹配前面是数字的a</span><br><span class="line"></span><br><span class="line">(?&lt;!正则表达式)</span><br><span class="line"># （?&lt;!\d）a 匹配前面不是数字的a</span><br><span class="line"></span><br><span class="line">(?=正则表达式)</span><br><span class="line"># a(?=\d)  后面是数字的a</span><br><span class="line"></span><br><span class="line">(?!正则表达式)</span><br><span class="line"># a(?!\d)  后面不等于数字的a</span><br><span class="line"></span><br><span class="line">(?#...)</span><br><span class="line"># 作为注释</span><br><span class="line"></span><br><span class="line">(?:正则表达式)</span><br><span class="line"># 同（...）</span><br><span class="line"></span><br><span class="line">|</span><br><span class="line"># 先匹配左边表达式，一旦成功就跳过右边表达式</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(?iLmsux)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(?(id/name)yes-pattern|no-pattern)</span><br><span class="line"># (\d)abc(?(1)\d|abc) 可匹配至 1abc2 和 abcabc</span><br><span class="line"># 若编号为?(id/name)组匹配到字符串，则执行yes-pattern匹配，否则执行no-pattern匹配</span><br><span class="line"></span><br><span class="line">贪婪模式       </span><br><span class="line">ab*  匹配 abbbc</span><br><span class="line"></span><br><span class="line">非贪婪模式     </span><br><span class="line">ab*?  匹配 a</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com2019/01/13/python入门/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Marcus Anthony">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/mar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Marcus Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2019/01/13/python入门/" itemprop="url">python入门</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-13T19:40:56+08:00">
                2019-01-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Python：一种交互式、解释性、面向对象的编程语言"><a href="#Python：一种交互式、解释性、面向对象的编程语言" class="headerlink" title="Python：一种交互式、解释性、面向对象的编程语言"></a>Python：一种交互式、解释性、面向对象的编程语言</h3><p>解释性语言：即无需编译源码为可执行文件，直接使用源码就可以运行</p>
<h4 id="Python-shell"><a href="#Python-shell" class="headerlink" title="Python shell"></a>Python shell</h4><p>提供一个运行环境，方便交互式开发。在windows下，分为两种，Python(command line) 和 IDLE</p>
<h4 id="Python-IDE"><a href="#Python-IDE" class="headerlink" title="Python IDE"></a>Python IDE</h4><p>将Python开发相关的各种工具集成，包括python代码编辑器，python运行环境（pyhton shell）。常见主流的IDE有Pycharm和spyder</p>
<h4 id="python编码"><a href="#python编码" class="headerlink" title="python编码"></a>python编码</h4><ul>
<li><strong>字节与字符</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">计算机存储的一切数据，文本字符、图片、视频、音频、软件都是由一串01的字节序列构成的，一个字节等于8个比特位。</span><br><span class="line"></span><br><span class="line">而字符就是一个符号，比如一个汉字、一个英文字母、一个数字、一个标点都可以称为一个字符。</span><br><span class="line"></span><br><span class="line">字节方便存储和网络传输，而字符用于显示，方便阅读。例如字符 &quot;p&quot; 存储到硬盘是一串二进制数据 01110000，占用一个字节的长度</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>len() 函数</strong> </li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">len(string)返回的是字节数，len(unicode)返回的是字符数</span><br><span class="line"></span><br><span class="line">gbk编码每个汉字占用2个字节，utf8编码的每个汉字占用3个字节,而1个英文字符只占用1个字节</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>编码与解码</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们用编辑器打开的文本，看到的一个个字符，最终保存在磁盘的时候都是以二进制字节序列形式存起来的。那么从字符到字节的转换过程就叫做编码（encode），反过来叫做解码（decode），两者是一个可逆的过程。编码是为了存储传输，解码是为了方便显示阅读</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>ascill编码</strong>   </li>
</ul>
<p>英文字母加上特殊字符，一共128个字符。这个就是ascill编码。对应关系很简单，一个字符对应一个byte</p>
<ul>
<li><strong>Unicode编码</strong></li>
</ul>
<p>Unicode（统一码，万国码）是基于通用字符集（Universal Character Set）的标准发展。它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足语言、跨平台进行文本转换、处理的要求</p>
<ul>
<li><strong>UTF-8</strong></li>
</ul>
<p>Unicode是一个标准，Unicode只是一个符号集，它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储；UTF-8是实现，UTF-8以字节为单位对Unicode进行编码<br>UTF-8（8-bit Unicode Transformation Format）是一种针对Unicode的可变长度字符编码（定长码），也是一种前缀码。它可以用来表示 Unicode 标准中的任何字符，且其编码中的第一个字节仍与<a href="">ASCII</a>兼容<br>windows记事本保存的utf-8格式，开始的三个字节EF BB BF就是BOM了，全称Byte Order Mark，这玩意也是很多乱码问题的来源，Linux下很多程序就不认BOM。因此，强烈不建议使用BOM，使用Notepad++之类的软件保存文本时，尽量选择以UTF-8无BOM格式编码</p>
<ul>
<li><strong>linux 转UTF-8(无bom)</strong></li>
</ul>
<p>用 Vim 打开</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">:set nobomb</span><br><span class="line">:wq</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li><strong>Python2</strong></li>
</ul>
<p>Python默认脚本文件都是ASCII编码的，当文件中有非ASCII编码范围内的字符的时候就要使用“编码指示”来修正，也就是在文件第一行或第二行指定编码声明：<br><code># -*- coding=utf-8 -*</code>-或者<code>#coding=utf-8</code>，若头部声明coding=utf-8, a = ‘中文’ 其编码为utf-8，若头部声明coding=gb2312, a = ‘中文’ 其编码为gbk</p>
<p>python解释器纯粹把源码使用ascii编码进行解析生成语法树。考虑到源码里可能存在其他语言的字符串量，提供了setdefaultencode接口，但是非常容易引发各类问题。PEP263指出在文件第一行或者第二行（仅限第一行为Unix脚本标注的情况下）写入特殊格式的注释# coding:xxx可以指定解释器解释源码时使用的字符编码</p>
<p>Python2 把字符串分为 unicode 和 str 两种类型。本质上 str 是一串二进制字节序列，我们要把 unicode 符号保存到文件或者传输到网络就需要经过编码处理转换成 str 类型</p>
<p><strong>python2的print的实质是将str里的东西输出到PIPE，如果你print的是一个unicode对象，它会自动根据LOCALE环境变量进行encode之后变成str再输出。然而一般在Windows上都没有设置locale环境变量，py2就按照默认的ascii编码进行处理，于是对中文自然就编码错误了</strong>。解决方法是手动encode成对应的输出端可接受的编码后输出。win下一般都是gbk，linux下一般都是utf8</p>
<hr>
<ul>
<li><strong>str和unicode对象的转换，通过encode和decode实现</strong></li>
</ul>
<p>str对象,存储 bytes，它仅仅是一个字节流，没有其它的含义，如果你想使这个字节流显示的内容有意义，就必须用正确的编码格式，解码显示</p>
<p>如果你使用一个 “u” 前缀，那么你会有一个 “unicode” 对象，存储的是 code points ，在一个 unicode 字符串中，你可以使用反斜杠 u(u) 来插入任何的 unicode 代码点</p>
<p>Unicode 字符串会有一个 .encode 方法来产生 bytes , bytes 串会有一个 .decode 方法来产生 unicode 。每个方法中都有一个参数来表明你要操作的编码类型</p>
<p><strong>decode方法是将一个str按照指定编码解析后转换为unicode，encode方法则是把一个unicode对象用指定编码表示并存储到一个str对象里</strong></p>
<ul>
<li><strong>字符串前加u如何理解：</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">两种字符串如何相互转换？字符串&apos;xxx&apos;虽然是ASCII编码，但也可以看成是UTF-8编码，而u&apos;xxx&apos;则只能是Unicode编码</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>如何获得系统默认编码</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">print sys.getdefaultencoding()</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Python中用encoding声明的文件编码和文件的实际编码之间</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">关键是保持 编码和解码时用的编码类型一致</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;x=u&quot;禅&quot;</span><br><span class="line">&gt;&gt;&gt;a=x.encode(&quot;utf-8&quot;)</span><br><span class="line">&gt;&gt;&gt;a</span><br><span class="line">&apos;\xe7\xa6\x85&apos;</span><br><span class="line">&gt;&gt;&gt;a.decode(&quot;gbk&quot;)</span><br><span class="line">出现错误，需用decode(&quot;utf-8&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>开头声明编码格式的解释</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.如果没有此文件编码类型的声明，则python默认以ASCII编码去处理</span><br><span class="line"></span><br><span class="line">2.必须放在python文件的第一行或第二行</span><br><span class="line"></span><br><span class="line">3.程序会通过头部声明，解码初始化 u”字符串”，这样的unicode对象，所以头部声明和代码的存储格式要一致</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">str = u.encode(&apos;utf-8&apos;) </span><br><span class="line">#以utf-8编码对unicode对象进行编码</span><br><span class="line"></span><br><span class="line">u = str.decode(&apos;gb2312&apos;)</span><br><span class="line">#以gb2312编码对字符串str进行解码，以获取unicode</span><br></pre></td></tr></table></figure>
<p>eg.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;su=u&apos;哈哈&apos;</span><br><span class="line">&gt;&gt;&gt;type(su)</span><br><span class="line">&lt;type &apos;unicode&apos;&gt;</span><br><span class="line">&gt;&gt;&gt;s_utf8=su.encode(&apos;utf-8&apos;)</span><br><span class="line">&gt;&gt;&gt;type(s_utf8)</span><br><span class="line">&lt;type &apos;str&apos;&gt;</span><br><span class="line">&gt;&gt;&gt;s_utf8</span><br><span class="line">&apos;\xe5\x93\x88\xe5\x93\x88&apos;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>当将一个unicode对象传给print时，在内部会将该unicode对象进行一次转换，转换成本地的默认编码</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;print s_utf8</span><br><span class="line">乱码显示</span><br><span class="line">&gt;&gt;&gt;print su</span><br><span class="line">哈哈</span><br><span class="line">&gt;&gt;&gt;s_gbk=su.encode(&apos;gbk&apos;)</span><br><span class="line">&gt;&gt;&gt;s_gbk</span><br><span class="line">&apos;\xb9\xfe\xb9\xfe&apos;</span><br><span class="line">&gt;&gt;&gt; print s_gbk</span><br><span class="line">哈哈</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com2018/12/13/sklearn模块/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Marcus Anthony">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/mar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Marcus Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/12/13/sklearn模块/" itemprop="url">python之旅</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-13T15:28:52+08:00">
                2018-12-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Numpy模块"><a href="#Numpy模块" class="headerlink" title="Numpy模块"></a>Numpy模块</h1><h3 id="array"><a href="#array" class="headerlink" title="array"></a>array</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">np.array() 创建N维数组对象，元素必须相同类型，每个数组都有一个shape和一个dyte</span><br><span class="line">**和列表最重要的区别，数组切片是原始数组的视图，即视图上的任何修改都会直接反映到源数组上，如果需要的是一份副本而非视图，即操作arr[:].copy()**</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">常用属性：</span><br><span class="line">np.nan；-np.inf；np.inf；</span><br><span class="line">np.arange()；np.ones()；np.mat() ；np.zeros((2,2))；np.eyes(4)</span><br><span class="line">np.reshape(-1,1) #转化为一列，-1代表自动计算行数</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">常用合并：</span><br><span class="line">np.vstack() #对array进行上下合并</span><br><span class="line">np.hstack() #对array进行横向合并</span><br><span class="line">np.r_[a,b,c] #类似pandas中的concat</span><br><span class="line">np.c_[a,b,c] #类似pandas中的merge</span><br><span class="line">np.column_stack()# 类似hstack，将每个元素作为一列</span><br><span class="line">np.concatenate([a,b],axis=1) # 对array进行合并,同hstack()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.sort(array, axis, kind, order) # 返回新数组</span><br><span class="line">np.diff() #数组相邻两个元素之间的差</span><br><span class="line">np.split() # 如果是一个整数，就用该数平均切分，如果是一个数组，为沿轴切分的位置</span><br><span class="line">np.repeat([,axis]) #将数组中的各个元素重复一定次数</span><br><span class="line">np.tile() #堆叠数组副本</span><br><span class="line">a[:, np.newaxis] # 给a最外层中括号中的每一个元素加[]</span><br><span class="line">a[newaxis, :] # 给a最外层中括号中所有元素加[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">随机取数：</span><br><span class="line">np.linspace(0,10,50)</span><br><span class="line">#返回50个均匀分布的样本，在[0, 10]之间</span><br><span class="line"></span><br><span class="line">numpy.random.randn(d0, d1, ..., dn)</span><br><span class="line">#是从标准正态分布中返回一个或多个样本值，dn指维数</span><br><span class="line"></span><br><span class="line">numpy.random.rand(d0, d1, …, dn)</span><br><span class="line">#随机样本位于[0, 1)中 ，dn指维数</span><br><span class="line"></span><br><span class="line">numpy.random.randint(low,high=None,size)</span><br><span class="line">#生成在[low,high) 之间均匀分布的样本，若high为none，区间为[0，low)</span><br><span class="line"></span><br><span class="line">np.random.normal(mean,stdev,size)</span><br><span class="line">#均值为mean，标准差为stdev，返回size个高斯随机数</span><br><span class="line"></span><br><span class="line">numpy.random.choice(a, size=None, replace=True, p=None)</span><br><span class="line">从a中随机选取size个数量，replace为True时，采样会重复</span><br></pre></td></tr></table></figure>
<h3 id="集合运算"><a href="#集合运算" class="headerlink" title="集合运算"></a>集合运算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">intersect1d(x,y) 返回x、y公共元素</span><br><span class="line">union1d(x,y)  返回x、y并集</span><br><span class="line">in1d(x,y)  返回x元素是否在y中</span><br><span class="line">setdiff1d(x,y) 集合差，含于x，不含y</span><br><span class="line">setxor1d(x,y) x、y中非并集的元素</span><br></pre></td></tr></table></figure>
<h3 id="索引运算"><a href="#索引运算" class="headerlink" title="索引运算"></a>索引运算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">names[names==&apos;a&apos;]</span><br><span class="line">data=np.random.randn(2,4)</span><br><span class="line">data[data&lt;0]=0</span><br><span class="line">arr[2,0] 与 arr[2][0] 等价</span><br><span class="line">np.where(conditions,x,y) # 条件判断</span><br><span class="line">np.where(conditions) # 返回输入数组中满足给定条件的元素的索引</span><br></pre></td></tr></table></figure>
<h3 id="函数运算"><a href="#函数运算" class="headerlink" title="函数运算"></a>函数运算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import numpy.linalg</span><br><span class="line">diag() #返回方阵的对角线元素，或将一维数组转换为方阵</span><br><span class="line">dot() #秩为1的数组，执行对应位置相乘，然后再相加,秩不为1的二维数组，执行矩阵乘法运算</span><br><span class="line">星号（*）乘法运算 # 对数组执行对应位置相乘,对矩阵执行矩阵乘法运算</span><br><span class="line">norm() #求范数，默认为L2范数</span><br><span class="line">multiply() #对应元素位置相乘</span><br><span class="line">trace() #计算对角线元素和</span><br><span class="line">det() #计算矩阵行列式</span><br><span class="line">eig() #计算特征值和特征向量</span><br><span class="line">inv() #计算方阵的逆</span><br><span class="line">qr() #计算QR分解</span><br><span class="line">svd() #计算奇异值分解</span><br><span class="line">solve() #解线性方程组Ax=b</span><br><span class="line">lstsq() #计算Ax=b的最小二乘解</span><br><span class="line"></span><br><span class="line">np.dot() # 矩阵乘法，计算矩阵内积</span><br></pre></td></tr></table></figure>
<h1 id="Pandas模块"><a href="#Pandas模块" class="headerlink" title="Pandas模块"></a>Pandas模块</h1><h3 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">delq = pd.Series([1,0,2,3,0])</span><br><span class="line">delq = pd.Series([1,0,2,3,0],index=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;])</span><br><span class="line">delq.reindex([&apos;b&apos;,&apos;a&apos;,&apos;d&apos;,&apos;c&apos;],fill_value=0)</span><br><span class="line">delq =pd.Series(&#123;&apos;a&apos;:1,&apos;b&apos;:0,&apos;c&apos;:2,&apos;d&apos;:3,&apos;e&apos;:0&#125;)</span><br><span class="line">print delq.values</span><br><span class="line">print delq.index</span><br><span class="line">print delq.dtypes</span><br><span class="line">print delq.get_dtype_counts()</span><br><span class="line">print delq.index.tolist()</span><br><span class="line">print delq[2]</span><br><span class="line">print delq[delq!=0]</span><br></pre></td></tr></table></figure>
<h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">一个表格型的数据结构。它提供有序的列和不同类型的列值,运算时，会自动对齐行和列，没有重叠引入NaN值</span><br><span class="line">DataFrame的列获取为Series</span><br><span class="line">spend = pd.read_csv(&apos;spend.csv&apos;,header = 0)</span><br><span class="line">spend.head()</span><br><span class="line">spend.info()</span><br><span class="line">spend.describe()</span><br><span class="line">spend.select_dtypes(include,exclude)</span><br><span class="line">spend._get_numeric_data() #drop non-numeric cols</span><br><span class="line">spend = pd.DataFrame(&#123;&apos;a&apos;:list(range(10)),&apos;b&apos;:list(range(20,10,-1))&#125;)</span><br><span class="line">pd.DataFrame.from_dict(data[,orient=&apos;index&apos;]) 字典转化为dataframe</span><br><span class="line">spend=pd.DataFrame(np.random.randn(4,3),columns=[&apos;one&apos;,&apos;two&apos;,&apos;three&apos;],index=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;])</span><br><span class="line">spend[&apos;one&apos;][&apos;a&apos;]=spend.loc[&apos;a&apos;,&apos;one&apos;]=spend.loc[&apos;a&apos;][&apos;one&apos;]</span><br><span class="line">spend.loc[:,&apos;one&apos;]=spend[&apos;one&apos;]</span><br><span class="line">spend.loc[&apos;a&apos;]=spend.loc[&apos;a&apos;,:]</span><br><span class="line">spend[(spend&gt;n).all(1)] 全部符合条件的行</span><br><span class="line">spend[(spend&gt;n).all(0)] 全部符合条件的列</span><br></pre></td></tr></table></figure>
<h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">spend.iloc[3]</span><br><span class="line">#选取第3条记录</span><br><span class="line">spend.ix[3]</span><br><span class="line">#ix可以通过行号和行标签进行索引，而iloc只能通过行号索引,loc只通过行标签索引</span><br><span class="line">spend[(spend.a&gt;5)&amp;(spend.b&lt;=15)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spend.index.is_unique</span><br><span class="line"># 判断索引是否有重复值</span><br><span class="line"></span><br><span class="line">spend.replace(1,&apos;one&apos;)</span><br><span class="line">#用‘one’代替所有等于1的值</span><br><span class="line">spend.replace([1,3],[&apos;one&apos;,&apos;three&apos;])</span><br><span class="line">#用&apos;one&apos;代替1，用&apos;three&apos;代替3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spend.rename(columns=&#123;&apos;old_name&apos;: &apos;new_ name&apos;&#125;)</span><br><span class="line">#选择性更改列名</span><br><span class="line">spend.rename(index=lambda x: x + 1)</span><br><span class="line">#批量重命名索引</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spend.set_index(&apos;column_one&apos;)</span><br><span class="line">#将某列设为索引</span><br><span class="line">spend.reset_index(drop=True,name=)</span><br><span class="line">#重新设定索引列，删除原索引列</span><br><span class="line">spend.reindex(index=[],columns=[],method=&apos;ffill&apos;)</span><br><span class="line">spend.reindex(spend.index.difference([]))</span><br><span class="line"># 重构索引</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spend.drop(index=[])</span><br><span class="line">spend.drop_duplicates(subset=None, keep=&apos;first&apos;, inplace=False)</span><br><span class="line">#去除特定列下面的重复行</span><br><span class="line"></span><br><span class="line">spend.drop(column_name,axis=1)</span><br><span class="line">#Use axis=1（跨列） to apply a method across each row, or to the column labels.</span><br><span class="line">#Use axis=0（跨行）to apply a method down each column, or to the row labels (the index).</span><br></pre></td></tr></table></figure>
<h3 id="函数应用"><a href="#函数应用" class="headerlink" title="函数应用"></a>函数应用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">spend.sum(axis=1,skipna=False)</span><br><span class="line">spend.mode() 众数</span><br><span class="line">spend[col].unique() </span><br><span class="line">spend.nunique() 获取去重值</span><br><span class="line">spend.idxmax() 获取最大值的索引值</span><br><span class="line">spend.cumsum() 每一列的累加和</span><br><span class="line">spend.insert(loc,col,value) 插入列</span><br><span class="line">spend[col].value_counts() 返回各值频率</span><br><span class="line">spend.apply(pd.Series.value_counts) 查看DataFrame对象中每一列的唯一值和计数</span><br><span class="line"></span><br><span class="line">cut将根据值本身来选择箱子均匀间隔，qcut是根据这些值的频率来选择箱子的均匀间隔</span><br><span class="line">pd.qcut(spend,n,labels=[])</span><br><span class="line">pd.cut(spend,n,labels=[])</span><br><span class="line"></span><br><span class="line">transform同一时间在一个Series上进行一次转换，返回与 group相同的单个维度的序列</span><br><span class="line">spend.transform(&#123;col1: func, col2: func&#125;)</span><br><span class="line"></span><br><span class="line">col前n个最大小值</span><br><span class="line">spend.nlargest(n, col)</span><br><span class="line">spend.nsmallest(n, col)</span><br><span class="line"></span><br><span class="line">apply函数：让函数作用在dataframe某一维的向量</span><br><span class="line">spend[‘a’].apply(lambda x: x+1)</span><br><span class="line">spend.iloc[3].apply(lambda x: x+1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">applymap函数：让函数作用在dataframe的每一个元素上</span><br><span class="line">spend.applymap(lambda x: x+1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spend.groupby([col1,col2])：返回一个按多列进行分组的Groupby对象</span><br><span class="line">spend.groupby(col1，group_keys=False)[col2].agg([&apos;mean&apos;,&apos;sum&apos;])</span><br><span class="line"># 返回按列col1分组的所有列的均值,和.group_keys 禁止分组的键</span><br><span class="line"></span><br><span class="line">spend.groupby(col1).apply(np.mean)</span><br><span class="line"># apply应用各列，agg仅作用于指定的列,agg可以传入多个函数</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spend.pivot_table(index=col1, values=[col2,col3], aggfunc=max)</span><br><span class="line">#创建一个按列col1进行分组，并计算col2和col3的最大值的数据透视表</span><br><span class="line"></span><br><span class="line">pd.get_dummies(data, prefix=None, prefix_sep=&apos;_&apos;, dummy_na=False, columns=None, sparse=False, drop_first=False)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">from patsy import dmatrices</span><br><span class="line">#构建线性模型矩阵</span><br><span class="line">y, X = patsy.dmatrices(&apos;y ~ x0 + x1&apos;, data，return_type=&apos;dataframe&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spend.corr()</span><br><span class="line">spend.corrwith(data.X)</span><br><span class="line"># 变量间的相关系数，和某个确定变量的相关系数</span><br><span class="line"></span><br><span class="line">spend.melt(id_vars=None, value_vars=None, var_name=None, value_name=&apos;value&apos;, col_level=None)</span><br><span class="line"># 透视操作，id_vars是指普通列的列名，value_vars是指那些需要转换的列名，转化为variable和value列</span><br><span class="line"></span><br><span class="line">spend.stack(self, level=-1, dropna=True)</span><br><span class="line">#行头变为列头,参数level指向行索引值，或行索引名称</span><br><span class="line"></span><br><span class="line">spend.unstack(self, level=-1, fill_value=None)</span><br><span class="line">#unstack，可使得列头变为行头，参数level指向列索引值，或列索引名称</span><br></pre></td></tr></table></figure>
<h3 id="排序和合并"><a href="#排序和合并" class="headerlink" title="排序和合并"></a>排序和合并</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">spend.sort_index([axis=1],[ascending=False],[by=column_name])</span><br><span class="line"></span><br><span class="line">spend.sort_values(by = column_name,[ascending = True],[na_position=&apos;first&apos;])</span><br><span class="line"></span><br><span class="line">pd.merge(df1,df2,how=[inner,outer,left,right],left_on=&apos;&apos;,right_on=&apos;&apos;,on=[key1,key2],left_index=false,right_index=false)</span><br><span class="line"></span><br><span class="line">pd.concat([s1,s2,s3],axis=，ignore_index=True)</span><br><span class="line">ignore_index 产生新的索引</span><br></pre></td></tr></table></figure>
<h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pandas默认使用NaN表示缺失数据；</span><br><span class="line">dropna默认丢弃任何含有缺失值的行；</span><br><span class="line">dropna(how=&apos;all&apos;)只丢弃全为NA的行</span><br><span class="line">fillna(,[inplace=True])替换缺失值,inplace为true直接修改原对象</span><br><span class="line">fillna(&#123;1:0.5,3:-1&#125;)</span><br><span class="line">isnull() 返回一个布尔值对象，该对象类型与源类型一样</span><br></pre></td></tr></table></figure>
<h1 id="Scikit-learn-模块"><a href="#Scikit-learn-模块" class="headerlink" title="Scikit-learn 模块"></a>Scikit-learn 模块</h1><p>Scikit-learn的基本功能主要被分为六大部分：分类，回归，聚类，数据降维，模型选择和数据预处理</p>
<h3 id="datasets"><a href="#datasets" class="headerlink" title="datasets"></a>datasets</h3><p>调用自带数据库中的数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. datasets.load_*()</span><br><span class="line">2. datasets.fetch_*()</span><br><span class="line">3. datasets.make_*()</span><br></pre></td></tr></table></figure></p>
<p>datasets获取对象的属性：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data:数据集</span><br><span class="line">target：数据对应的类标记</span><br><span class="line">target_name:类标记对应的名字</span><br><span class="line">DESCR:数据集的描述信息</span><br></pre></td></tr></table></figure></p>
<h3 id="preprocessing"><a href="#preprocessing" class="headerlink" title="preprocessing"></a>preprocessing</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">preprocessing.scale()</span><br><span class="line"></span><br><span class="line">preprocessing.StandardScaler().fit_transform()</span><br><span class="line">将数据都聚集在均值0附近，方差值为1。</span><br><span class="line"></span><br><span class="line">preprocessing.MinMaxScaler().fit_transform()</span><br><span class="line">将数据矩阵缩放到``[0, 1]``，对于方差非常小的属性可以增强其稳定性；可以维持稀疏矩阵中为0的条目</span><br><span class="line"></span><br><span class="line">preprocessing.MaxAbsScaler.fit_transform()</span><br><span class="line">将数据矩阵缩放到``[-1, 1]``</span><br><span class="line"></span><br><span class="line">preprocessing.normalize((X, norm=&apos;l2&apos;))</span><br><span class="line">归一化的过程是将每个样本缩放到单位范数，使用L1或L2范数，主要思想是对每个样本计算范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的范数等于1</span><br><span class="line"></span><br><span class="line">preprocessing.Binarizer().fit_transform(X)</span><br><span class="line">特征二值化，默认阈值为0，大于阈值的分类1，小于阈值的分类0</span><br><span class="line"></span><br><span class="line">preprocessing.Imputer(missing_values=&apos;NaN&apos;, strategy=&apos;mean&apos;, axis=0).fit()</span><br><span class="line">缺失值插补</span><br><span class="line"></span><br><span class="line">preprocessing.RobustScaler()</span><br><span class="line">根据第1个四分位数和第3个四分位数之间的范围来缩放数据</span><br></pre></td></tr></table></figure>
<h5 id="二值化编码"><a href="#二值化编码" class="headerlink" title="二值化编码"></a>二值化编码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">OneHotEncoder(n_values=None, categorical_features=None, categories=None, sparse=True, dtype=&lt;class ‘numpy.float64’&gt;, handle_unknown=’error’)</span><br><span class="line">无法对文本编码，且输入必须是二维</span><br><span class="line">sparse：若为True时，返回稀疏矩阵，否则返回数组</span><br><span class="line">categorical_features：若为all，代表所有的特征都被视为分类特征</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">LabelEncoder(), </span><br><span class="line">对不连续的数值或文本进行编码，且输入必须为一维</span><br></pre></td></tr></table></figure>
<h3 id="feature-selection"><a href="#feature-selection" class="headerlink" title="feature_selection"></a>feature_selection</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">VarianceThreshold(threshold)</span><br><span class="line">通过特征的方差来提取特征，默认方差为0的特征会自动删除</span><br><span class="line"></span><br><span class="line">SelectKBest(score_func, k=10)</span><br><span class="line">from minepy import MINE</span><br><span class="line">from sklearn.feature_selection import chi2</span><br><span class="line">from scipy.stats import pearsonr</span><br><span class="line">scores按升序排序，选择排前k名所对应的特征,</span><br><span class="line">score_func 通常结合 卡方检验chi2，Pearson相关系数 pearsonr,最大信息系数 MINE</span><br></pre></td></tr></table></figure>
<h3 id="model-selection"><a href="#model-selection" class="headerlink" title="model_selection"></a>model_selection</h3><p>主要提供交叉验证和结果评估的工具</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">train_test_split(*array,test_size=0.25,train_size=None,random_state=None,shuffle=True,stratify=None)</span><br><span class="line"># 返回切分的数据集,默认test_size将被设置为0.25，train_size将被设置为0.75，stratify按比例抽取训练集和测试集，random_state不同值获取到不同的数据集</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cross_val_score(estimator,raw_data,raw_target,cv,scoring)</span><br><span class="line"># 返回train/test数据集上的每折得分,estimator为分类器，raw_data为原数据，raw_target为原数据类别，cv默认为3折验证,scoring为评分算法</span><br><span class="line"></span><br><span class="line">GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, refit=True, cv=’warn’)</span><br><span class="line">#自动调参，只要把参数输进去，就能给出最优化的结果和参数。但是这个方法适合于小数据集，可能会调到局部最优而不是全局最优</span><br><span class="line">param_grid：值为字典或者列表，即需要最优化的参数的取值</span><br><span class="line">score：评价标准，默认None,如scoring=&apos;roc_auc&apos;</span><br><span class="line">refit搜索参数结束后，用最佳参数结果再次fit一遍全部数据集</span><br><span class="line">grid.fit()：运行网格搜索</span><br><span class="line">grid_scores_：给出不同参数情况下的评价结果</span><br><span class="line">best_params_：描述了已取得最佳结果的参数的组合</span><br><span class="line">best_score_：成员提供优化过程期间观察到的最好的评分</span><br></pre></td></tr></table></figure>
<h5 id="k折交叉划分"><a href="#k折交叉划分" class="headerlink" title="k折交叉划分"></a>k折交叉划分</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kf=KFold(n_splits=3, shuffle=False, random_state=None)</span><br><span class="line">for train_index,test_index in kf.split():</span><br><span class="line"># 将数据集m划分n_splits个不相交子集，每个子集有m/n_splits个训练样例，每次使用k-1个子集作为训练数据，用1个子集作为测试数据，训练k次。最终的结果是这k次测试结果的均值。返回数据集index</span><br><span class="line"></span><br><span class="line">StratifiedKFold用法类似Kfold，但是他是分层采样，确保训练集，测试集中各类别样本的比例与原始数据集中相同</span><br><span class="line">LabelKFold与StratifiedKFold用法相反</span><br></pre></td></tr></table></figure>
<h5 id="随机划分法"><a href="#随机划分法" class="headerlink" title="随机划分法"></a>随机划分法</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ShuffleSplit()</span><br><span class="line"># 首先对样本全体随机打乱，然后再划分出train/tes对，比KFold交叉更好的控制train/test比例</span><br><span class="line"></span><br><span class="line">ss=StratifiedShuffleSplit(n_splits=10,test_size=None,train_size=None, random_state=None)</span><br><span class="line">for train_index, test_index in ss.split(X, y):</span><br><span class="line"># n_splits是将训练数据分成train/test对的组数，StratifiedShuffleSplit是ShuffleSplit的一个变体，返回分层划分</span><br></pre></td></tr></table></figure>
<h3 id="metrics"><a href="#metrics" class="headerlink" title="metrics"></a>metrics</h3><p>用于评估方法中，衡量 分类器性能</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">accuracy_score(y_true,y_pred) 计算 accuracy</span><br><span class="line"></span><br><span class="line">二分类指标：</span><br><span class="line">precision_recall_curve（y_true,y_score）</span><br><span class="line">fpr,tpr,thresholds=roc_curve(y_true, y_score)</span><br><span class="line">y_score 表示每个测试样本属于正样本的概率，从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本，每次选取一个不同的threshold，我们就可以得到一组FPR和TPR，即ROC曲线上的一点</span><br><span class="line"></span><br><span class="line">roc_auc_score(y_true, y_score, average=&apos;macro&apos;, sample_weight=None) 计算预测得分曲线下的面积</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">confusion_matrix(y_true, y_pred, labels=None, sample_weight=None) 输出为混淆矩阵</span><br></pre></td></tr></table></figure>
<h3 id="linear-model"><a href="#linear-model" class="headerlink" title="linear_model"></a>linear_model</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">LogisticRegression(penalty,dual,C,solver,tol,max_iter)</span><br><span class="line">penalty为正则化范数，默认为L2范数；dual为对偶或者原始方法，通常样本数大于特征数的情况下，默认为False；C为正则化系数λ的倒数，默认为1，值越小，代表正则化越强；solver参数决定了我们对逻辑回归损失函数的优化方法;tol为迭代终止判据的误差范围；max_iter为算法收敛的最大迭代次数</span><br><span class="line"></span><br><span class="line">LogisticRegression().fit(X, y, sample_weight=None)</span><br><span class="line">LogisticRegression().predict(X)</span><br><span class="line">LogisticRegression().score(X，Y) 返回准确率</span><br><span class="line">LogisticRegression().predict_proba(X) 返回每个样本每种类别的概率</span><br></pre></td></tr></table></figure>
<h3 id="tree"><a href="#tree" class="headerlink" title="tree"></a>tree</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DecisionTreeClassifier(criterion=&apos;gini&apos;, splitter=&apos;best&apos;, max_depth=None, min_samples_split=2,min_samples_leaf =1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None,class_weight=None, presort=False)</span><br><span class="line">CART分类回归树算法</span><br><span class="line">tree= DecisionTreeClassifier() 建立决策树模型</span><br><span class="line">tree.fit(X,Y) 构建实例</span><br><span class="line">tree.predict() 预测分类</span><br><span class="line">tree.predict_proba(X) 返回每个样本每种类别的概率</span><br></pre></td></tr></table></figure>
<h3 id="cluster"><a href="#cluster" class="headerlink" title="cluster"></a>cluster</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">estimator=KMeans(n_clusters,n_init=10,max_iter)</span><br><span class="line">n_cluster为簇的个数，n_init为获取初始簇质心迭代的次数，max_iter为最大迭代次数</span><br><span class="line"></span><br><span class="line">estimator.fit() 构建实例</span><br><span class="line">estimator.labels 获取聚类标签</span><br><span class="line">estimator.cluster_centers 获取聚类中心</span><br></pre></td></tr></table></figure>
<h3 id="ensemble"><a href="#ensemble" class="headerlink" title="ensemble"></a>ensemble</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">RandomForestClassifier(n_estimators,)</span><br><span class="line">n_estimators 决策树的个数，控制模型复杂度，bosstrap 是否有放回，oob_score 没有被boostrap选取的数据是否做验证，max_features 控制选取多少特征量，常用值为\sqrt&#123;n&#125;和log2(n) ,max_depth 控制子树的深度，min_samples_split控制内部节点再划分所需最小样本数，若节点样本数小于值，则不进行划分，min_samples_leaf 控制控制叶子节点最少样本数，若小于值，则和兄弟节点一起被剪枝</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">IsolationForest(n_estimators=100, max_samples=’auto’, contamination=0.1, max_features=1.0, bootstrap=False, n_jobs=1, random_state=None, verbose=0)</span><br><span class="line">max_samples默认采样数据256条样本</span><br><span class="line">contamination设置样本中异常点的比例</span><br><span class="line">fit(X)</span><br><span class="line">pretict(X) 返回1表示非异常值，-1为异常值</span><br><span class="line">decision_function(X) 返回样本的异常评分，值越小表示越有可能是异常样本</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">VotingClassifier(estimators=[(&apos;xgb&apos;, clf1), (&apos;rf&apos;, clf2), (&apos;svc&apos;, clf3)], voting=[&apos;hard&apos;,&apos;soft&apos;],weight=)</span><br><span class="line">针对分类问题的一种结合策略。基本思想是选择所有机器学习算法当中输出最多的那个类</span><br><span class="line">hard vote 硬投票是选择算法输出最多的标签，软投票是使用各个算法输出的类概率来进行类的选择，输入权重的话，会得到每个类的类概率的加权平均值</span><br></pre></td></tr></table></figure>
<h3 id="neighbors"><a href="#neighbors" class="headerlink" title="neighbors"></a>neighbors</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">KNeighborsClassifier(n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’, metric_params=None, n_jobs=None, **kwargs)</span><br><span class="line">KNN近邻分类，algorithm有三种，brute’对应第一种蛮力实现，‘kd_tree’对应第二种KD树实现，‘ball_tree’对应第三种的球树实现， ‘auto’则会在上面三种算法中做权衡，选择一个拟合最好的最优算法</span><br></pre></td></tr></table></figure>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">LinearSVC(penalty=&apos;l2&apos;, loss=&apos;squared_hinge&apos;, dual=True, tol=0.0001, C=1.0, multi_class=&apos;ovr&apos;, fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)</span><br><span class="line">LinearSVC是线性分类，不支持各种低维到高维的核函数，仅仅支持线性核函数，对线性不可分的数据不能使用</span><br><span class="line"></span><br><span class="line">NuSVC(nu=0.5, kernel=&apos;rbf&apos;, degree=3, gamma=&apos;auto&apos;, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=&apos;ovr&apos;, random_state=None)</span><br><span class="line"></span><br><span class="line">SVC(C=1.0, kernel=&apos;rbf&apos;, degree=3, gamma=&apos;auto&apos;, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=&apos;ovr&apos;, random_state=None)</span><br><span class="line"></span><br><span class="line">OneClassSVM(kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, tol=0.001, nu=0.5, shrinking=True, cache_size=200, verbose=False, max_iter=-1, random_state=None)</span><br><span class="line">pretict(X) 返回1表示非异常值，-1为异常值</span><br></pre></td></tr></table></figure>
<h3 id="decomposition"><a href="#decomposition" class="headerlink" title="decomposition"></a>decomposition</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PCA(n_components=None, copy=True, whiten=False)</span><br><span class="line">n_components:所要保留的主成分个数n</span><br></pre></td></tr></table></figure>
<h3 id="manifold"><a href="#manifold" class="headerlink" title="manifold"></a>manifold</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TSNE(n_components=2, perplexity=30.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=1000, n_iter_without_progress=300, min_grad_norm=1e-07, metric=’euclidean’, init=’random’, verbose=0, random_state=None, method=’barnes_hut’, angle=0.5)</span><br><span class="line">n_components:嵌入空间的维度</span><br></pre></td></tr></table></figure>
<h3 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h3><p>管道机制实现了对全部步骤的流式化封装和管理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Pipeline([(&apos;sc&apos;, StandardScaler()), (&apos;pca&apos;, PCA(n_components=2)),(&apos;clf&apos;, LogisticRegression(random_state=1))])</span><br><span class="line"></span><br><span class="line">make_pipeline(StandardScaler()，PCA(n_components=2)，LogisticRegression(random_state=1))</span><br><span class="line">pipline的简写</span><br></pre></td></tr></table></figure>
<h1 id="imblearn模块"><a href="#imblearn模块" class="headerlink" title="imblearn模块"></a>imblearn模块</h1><p>不平衡数据处理包</p>
<h3 id="Over-sampling"><a href="#Over-sampling" class="headerlink" title="Over_sampling"></a>Over_sampling</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RandomOverSampler(sampling_strategy=&apos;auto&apos;, return_indices=False, random_state=None, ratio=None)</span><br><span class="line">ros=RandomOverSampler()</span><br><span class="line">X_re,y_re=ros.fit_sample(X, y)</span><br><span class="line"></span><br><span class="line">SMOTE(sampling_strategy=&apos;auto&apos;, random_state=None, k_neighbors=5, m_neighbors=&apos;deprecated&apos;, out_step=&apos;deprecated&apos;, kind=&apos;deprecated&apos;, svm_estimator=&apos;deprecated&apos;, n_jobs=1, ratio=None)</span><br><span class="line">kind调参&#123;&apos;regular&apos;, &apos;borderline1&apos;, &apos;borderline2&apos;, &apos;svm&apos;&#125;调参判别k近邻的样本是否属于同类样本</span><br></pre></td></tr></table></figure>
<h3 id="Under-sampling"><a href="#Under-sampling" class="headerlink" title="Under_sampling"></a>Under_sampling</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RandomUnderSampler(sampling_strategy=&apos;auto&apos;, return_indices=False, random_state=None, replacement=False, ratio=None)</span><br><span class="line"></span><br><span class="line">NearMiss(sampling_strategy=&apos;auto&apos;, return_indices=False, random_state=None, version=1, n_neighbors=3, n_neighbors_ver3=3, n_jobs=1, ratio=None)</span><br><span class="line">version调参&#123;1，2，3&#125;调参判别K近邻样本选取</span><br></pre></td></tr></table></figure>
<h3 id="ensemble-1"><a href="#ensemble-1" class="headerlink" title="ensemble"></a>ensemble</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">EasyEnsembleClassifier(n_estimators=10, base_estimator=None, warm_start=False, sampling_strategy=&apos;auto&apos;, replacement=False, n_jobs=1, random_state=None, verbose=0)</span><br><span class="line">从多数类中抽样出和少数类数目差不多的样本，然后和少数类样本组合作为训练集。在这个训练集上学习一个adaboost分类器</span><br></pre></td></tr></table></figure>
<h1 id="xgboost-模块"><a href="#xgboost-模块" class="headerlink" title="xgboost 模块"></a>xgboost 模块</h1><h3 id="sklearn-XGBClassifier"><a href="#sklearn-XGBClassifier" class="headerlink" title="sklearn.XGBClassifier"></a>sklearn.XGBClassifier</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">from xgboost.sklearn import XGBClassifier</span><br><span class="line"></span><br><span class="line">XGBClassifier(learning_rate=0.1,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,subsample=0.8,colsample_bytree=0.8,objective=&apos;binary:logistic&apos;,nthread=4,scale_pos_weight=1,seed=27,reg_alpha=0, reg_lambda=1, colsample_bylevel=1)</span><br><span class="line"></span><br><span class="line">基本参数调优（learning_rate,n_estimators）</span><br><span class="line">决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree)</span><br><span class="line">正则化参数的调优(lambda, alpha)</span><br><span class="line"></span><br><span class="line">参数：booster：指定了用哪一种基模型。可以为：&apos;gbtree&apos;,&apos;gblinear&apos;,&apos;dart&apos;</span><br><span class="line">	gamma：最小划分损失min_split_loss。即对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。</span><br><span class="line">	max_depth:树的深度，默认值为6</span><br><span class="line">	min_child_weight： 一个整数，子节点的权重阈值。对于树模型（booster=gbtree,dart），权重就是：叶子节点包含样本的所有二阶偏导数之和。</span><br><span class="line">	subsample：一个浮点数，对训练样本的采样比例。默认值为 1 。如果为0.5，表示随机使用一半的训练样本来训练子树</span><br><span class="line">	colsample_bytree： 一个浮点数，构建子树时，对特征的采样比例。默认值为 1。</span><br><span class="line">	colsample_bylevel： 一个浮点数，寻找划分点时，对特征的采样比例。 默认值为 1。</span><br><span class="line">	reg_alpha： L1 正则化系数</span><br><span class="line">	reg_lambda： L2 正则化系数</span><br><span class="line">	scale_pos_weight： 用于调整正负样本的权重，常用于类别不平衡的分类问题。默认为 1。</span><br><span class="line">	</span><br><span class="line">&gt;&gt; objective </span><br><span class="line">回归任务</span><br><span class="line">reg:linear (默认)</span><br><span class="line">reg:logistic </span><br><span class="line">二分类</span><br><span class="line">binary:logistic     概率 </span><br><span class="line">binary：logitraw   类别</span><br><span class="line">多分类</span><br><span class="line">multi：softmax  num_class=n   返回类别</span><br><span class="line">multi：softprob   num_class=n  返回概率</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line">fit(X, y, sample_weight=None, eval_set=None, eval_metric=None,</span><br><span class="line">    early_stopping_rounds=None,verbose=True, xgb_model=None)</span><br><span class="line"></span><br><span class="line">参数：sample_weight： 一个序列，给出了每个样本的权重</span><br><span class="line">	eval_set： 一个列表，元素为(X,y)，给出了验证集及其标签</span><br><span class="line">	xgb_model：一个Booster实例，它给出了待训练的模型。</span><br><span class="line">	eval_metric： 一个字符串或者可调用对象，用于evaluation metric</span><br><span class="line">	early_stopping_rounds：在验证集上，当连续n次迭代，分数没有提高后，提前终止训练</span><br><span class="line"></span><br><span class="line">&gt;&gt; eval_metric</span><br><span class="line">回归任务(默认rmse) rmse--均方根误差;mae--平均绝对误差</span><br><span class="line">分类任务(默认error)</span><br><span class="line">auc--roc曲线下面积</span><br><span class="line">error--错误率（二分类）</span><br><span class="line">merror--错误率（多分类）</span><br><span class="line">logloss--负对数似然函数（二分类）</span><br><span class="line">mlogloss--负对数似然函数（多分类）</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">predict(data, output_margin=False, ntree_limit=0)</span><br><span class="line">参数：output_margin： 表示是否输出原始的、未经过转换的margin value</span><br><span class="line"></span><br><span class="line">predict_proba(data, output_margin=False, ntree_limit=0) ： 执行预测，预测的是各类别的概率</span><br><span class="line"></span><br><span class="line">evals_result()： 返回一个字典，给出了各个验证集在各个验证参数上的历史值</span><br></pre></td></tr></table></figure>
<h3 id="Xgboost"><a href="#Xgboost" class="headerlink" title="Xgboost"></a>Xgboost</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import xgboost </span><br><span class="line">xgboost.DMatrix(data, label=None, missing=None, weight=None, silent=False, feature_names=None, feature_types=None, nthread=None) </span><br><span class="line">无法识别object类型,需使用了sklearn.preprocessing中的LabelEncoder转化</span><br><span class="line">参数：label：一个序列，表示样本标记。，missing： 一个值，它是缺失值的默认值。，weight：一个序列，给出了数据集中每个样本的权重</span><br><span class="line">属性：feature_names： 返回每个特征的名字；feature_types： 返回每个特征的数据类型</span><br><span class="line">方法：.num_col()；.num_row()；.get_label();.get_weight()</span><br><span class="line"></span><br><span class="line">xgboost.train()： 使用给定的参数来训练一个booster</span><br><span class="line">xgboost.train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,</span><br><span class="line">   maximize=False, early_stopping_rounds=None, evals_result=None, verbose_eval=True,</span><br><span class="line">   xgb_model=None, callbacks=None, learning_rates=None)</span><br><span class="line">参数：dtrain：DMatrix对象，params： 键值对，num_boost_round： 表示boosting 迭代数量</span><br><span class="line">	evals： (DMatrix,string)验证集，以及验证集的名字，obj：表示自定义的目标函数</span><br><span class="line">	feval： 表示自定义的evaluation 函数，maximize： 如果为True，则表示是对feval 求最大值</span><br><span class="line">	learning_rates： 一个列表，给出了每个迭代步的学习率，</span><br><span class="line">	evals_result： 一个字典，它给出了对测试集要进行评估的指标</span><br><span class="line">	</span><br><span class="line">xgboost.cv()： 使用给定的参数执行交叉验证 </span><br><span class="line">xgboost.cv(params, dtrain, num_boost_round=10, nfold=3, stratified=False, folds=None,</span><br><span class="line">     metrics=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None,</span><br><span class="line">     fpreproc=None, as_pandas=True, verbose_eval=None, show_stdv=True, seed=0,</span><br><span class="line">     callbacks=None, shuffle=True)</span><br><span class="line">     参数：nfold： 表示交叉验证的fold 的数量，stratified： 如果为True，则执行分层采样</span><br><span class="line">     	folds： 一个scikit-learn 的 KFold 实例或者StratifiedKFold 实例</span><br><span class="line">     	shuffle： 如果为True，则创建folds 之前先混洗数据</span><br><span class="line">     	as_pandas： 如果为True，则返回DataFrame；否则返回ndarray</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/mar.jpg" alt="Marcus Anthony">
            
              <p class="site-author-name" itemprop="name">Marcus Anthony</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Marcus Anthony</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="https://cdn.staticfile.org/MathJax/MathJax-2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
